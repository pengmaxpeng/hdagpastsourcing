file_path,Semester,Case Name,Company Name,Brief Case Description,Detailed Case Description,Outcome,Programming Languages Used,Tech Stack,KPIs
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F20_Schneider_Case.pdf,F20,Case Name: Provide enhanced Indirect Procurement (IP) spend visibility,Schneider Electric,"Provide enhanced Indirect Procurement (IP) spend visibility by building on an existing platform (Prism) containing IP spend data to improve categorization integrity, enhance descriptive spend data analytics, enable predictive spend analytics via RPA or machine learning, and produce consolidated spend reporting that can accommodate all IP spend reporting needs across the Enterprise, customizable to Divisions.","Provide enhanced Indirect Procurement (IP) spend visibility. Build on an existing platform (Prism) containing IP spend data to improve categorization integrity, enhance descriptive spend data analytics, enable predictive spend analytics via RPA or machine learning, and produce consolidated spend reporting (one version of truth) that can accommodate all IP spend reporting needs across the Enterprise, customizable to Divisions. 1) enhanced data structure and data pull logic to accommodate “one version of truth” for all IP spend; 2) enable predictive analytics (definition and enablement) above and beyond enhancing the descriptive data analytics; 3) build on the data visualization foundation and explore what the next iteration can be; 4) explore how RPA and AI (machine learning) can add value by identifying patterns and opportunities to reduce the spend.",Outcome: Provide enhanced Indirect Procurement (IP) spend visibility and deep insights to employee help desk requests.,"**Programming Languages Used**: RPA, machine learning","**Tech Stack**: RPA, machine learning, data visualization",**KPIs**: enhanced data structure and data pull logic to accommodate “one version of truth” for all IP spend; enable predictive analytics (definition and enablement) above and beyond enhancing the descriptive data analytics; build on the data visualization foundation and explore what the next iteration can be; explore how RPA and AI (machine learning) can add value by identifying patterns and opportunities to reduce the spend.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F20_SIMON_Case.pdf,F20,Case Name: SIMON Markets LLC.,SIMON Markets LLC.,Reconcile SEC product terms with our internal records and reconcile data sent by clients against data stored in Excel files. Automate the extraction of data from various client Excel files and consolidate them into a single file.,Reconcile SEC product terms with our internal records. (a) scraping the SEC website (b) storing this data in a database (even sqlite to start is fine) (c) querying our internal APIs (d) identifying any inconsistencies in the data (e) alerting internal teams via email regarding any inconsistencies. No unit testing/deployment required. I estimate this would take about 1 week. Reconcile data sent by clients (within .pdfs) against data stored in Excel files. (a) parsing pdf files (b) reading Excel files (c) scanning for inconsistencies (d) alerting internal teams via email regarding any inconsistencies. Automate the extraction of data from various client Excel files (which arrive in different formats) and consolidate them into a single file. (a) parsing Excel files (b) data manipulation in python pandas (c) any error checking to ensure no data was omitted/duplicated,"Outcome: The project involved reconciling SEC product terms with internal records, reconciling client data from PDFs against Excel files, and automating the extraction and consolidation of data from various client Excel files.",Python,"Tech Stack: scraping, database (sqlite), internal APIs, email alerts, pdf parsing, Excel reading, python pandas","**KPIs**: Reconcile SEC product terms with internal records, reconcile data sent by clients against data stored in Excel files, automate the extraction of data from various client Excel files."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F20_NYCCompany_Case.pdf,F20,Case Name: NYC & Company,NYC & Company,"We will help build the ultimate dashboard for NYC tourism using ~30 data sources for the use of the NYC government (this will be seen by the mayor’s office) and NYC & Co members (restaurants, theatres etc.) around the following themes: Hotel Industry, Visitor Identification and Understanding, Economic Indicators (including spending and macroeconomic forecasts), Competitive Tracking, Office of Vital Statistics, Convention and Event Performance. The HDAG team will have support of a back end data engineer who may be able to help with the heavy lifting data manipulation work. Our work thus will be more focused on the front-end: designing and building the dashboard to make sure it is (1) aesthetic — matches NYC & Co’s status as the world’s top tourism marketing agency and (2) insightful for the user and not just a plain source of data. Please also note that this project’s workload will be on the heavier side. The project will entail using R, Tableau, Microsoft Publisher and there will be a lot of design work involved. If you rank NYC & Company as a preference, please highlight your skills experiences in any of the skills listed above.","We will help build the ultimate dashboard for NYC tourism using ~30 data sources for the use of the NYC government (this will be seen by the mayor’s office) and NYC & Co members (restaurants, theatres etc.) around the following themes: Hotel Industry, Visitor Identification and Understanding, Economic Indicators (including spending and macroeconomic forecasts), Competitive Tracking, Office of Vital Statistics, Convention and Event Performance. The HDAG team will have support of a back end data engineer who may be able to help with the heavy lifting data manipulation work. Our work thus will be more focused on the front-end: designing and building the dashboard to make sure it is (1) aesthetic — matches NYC & Co’s status as the world’s top tourism marketing agency and (2) insightful for the user and not just a plain source of data. Please also note that this project’s workload will be on the heavier side. The project will entail using R, Tableau, Microsoft Publisher and there will be a lot of design work involved. If you rank NYC & Company as a preference, please highlight your skills experiences in any of the skills listed above.","The project will help build the ultimate dashboard for NYC tourism using ~30 data sources for the use of the NYC government and NYC & Co members, focusing on themes such as Hotel Industry, Visitor Identification and Understanding, Economic Indicators, Competitive Tracking, Office of Vital Statistics, and Convention and Event Performance. The workload will be on the heavier side, involving R, Tableau, Microsoft Publisher, and significant design work.","R, Tableau, Microsoft Publisher","R, Tableau, Microsoft Publisher","**KPIs**: Hotel Industry, Visitor Identification and Understanding, Economic Indicators (including spending and macroeconomic forecasts), Competitive Tracking, Office of Vital Statistics, Convention and Event Performance"
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F20_UNIDO_Case.pdf,F20,"**Case Name**: UNIDO’s contribution to the Inclusive and Sustainable Industrial Development (ISID) and the Sustainable Development Goals (SDGs) through the lens of the Integrated results and performance framework (IRPF) and Medium-term programme framework, 2018-2021.",United Nations Industrial Development Organization,"Provide data visualization for the Strategy Department’s flagship project - visualize UNIDO’s contribution to the Inclusive and Sustainable Industrial Development (ISID) and the Sustainable Development Goals (SDGs) through the lens of the Integrated results and performance framework (IRPF) and Medium-term programme framework, 2018-2021.","Provide data visualization for the Strategy Department’s flagship project - visualize UNIDO’s contribution to the Inclusive and Sustainable Industrial Development (ISID) and the Sustainable Development Goals (SDGs) through the lens of the Integrated results and performance framework (IRPF) and Medium-term programme framework, 2018-2021.","Outcome: Provide data visualization for the Strategy Department’s flagship project - visualize UNIDO’s contribution to the Inclusive and Sustainable Industrial Development (ISID) and the Sustainable Development Goals (SDGs) through the lens of the Integrated results and performance framework (IRPF) and Medium-term programme framework, 2018-2021.",**Programming Languages Used**: None,Tech Stack: Not provided in the document.,"**KPIs**: Provide data visualization for the Strategy Department’s flagship project - visualize UNIDO’s contribution to the Inclusive and Sustainable Industrial Development (ISID) and the Sustainable Development Goals (SDGs) through the lens of the Integrated results and performance framework (IRPF) and Medium-term programme framework, 2018-2021."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F20_Bloom_Case.pdf,F20,Bloom,Bloom,"Bloom is an online education non-profit organization in China that runs education programs to provide remote education for over 200+ students living in unprivileged areas of China. They are connecting over 100+ tutors (from schools in US, China, and Canada) with corresponding schools in the villages, and teach them classes including but not limited to English, Math, etc.","Bloom is an online education non-profit organization in China that runs education programs to provide remote education for over 200+ students living in unprivileged areas of China. They are connecting over 100+ tutors (from schools in US, China, and Canada) with corresponding schools in the villages, and teach them classes including but not limited to English, Math, etc.","Bloom is an online education non-profit organization in China that runs education programs to provide remote education for over 200+ students living in unprivileged areas of China. They are connecting over 100+ tutors (from schools in US, China, and Canada) with corresponding schools in the villages, and teach them classes including but not limited to English, Math, etc.",**Programming Languages Used**: javascript,"Web development skills, Front-end, javascript, Back-end, Data system set-up",**KPIs**: Not explicitly mentioned in the document.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F21_Airbus_Case.pdf,F21,**Case Name**: OS-Climate,Airbus,"We will be working with the Chief Engineer of Digital Product Development and Manufacturing and his team with the goal of contributing to the open source project, OS-Climate. OS-Climate is a project that many Fortune 500 companies are contributing to in order to make climate data more accessible. Airbus has incentives to contribute to OS-Climate for a number of reasons. First, they want to do good for the world... but it’s a big company that wants to make money, and there are always other motives. The main reason is that there is a pact among similar companies to reduce CO2 emissions by 50% by 2050. In order to achieve this, it will require the contribution of many companies. Working solo to gain an edge is not an option with a goal this large.","We will be working with the Chief Engineer of Digital Product Development and Manufacturing and his team with the goal of contributing to the open source project, OS-Climate. OS-Climate is a project that many Fortune 500 companies are contributing to in order to make climate data more accessible. Airbus has incentives to contribute to OS-Climate for a number of reasons. First, they want to do good for the world... but it’s a big company that wants to make money, and there are always other motives. The main reason is that there is a pact among similar companies to reduce CO2 emissions by 50% by 2050. In order to achieve this, it will require the contribution of many companies. Working solo to gain an edge is not an option with a goal this large.",The end goal is to add this data to OS-Climate for public access.,**Programming Languages Used**: Not specified in the document.,**Tech Stack**: Not specified in the document.,"**KPIs**: Gather Data And Create Datasets, Create Pricing Model, Add data to OS-Climate for public access."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F21_HeadSpace_Case.pdf,F21,HeadSpace,Headspace,This Statement of Work (SOW) defines work to be performed by Harvard Data Analytics Group for Headspace through their Data Analytics and Business Intelligence team. This work consists of creating forecasting models on various metrics to help inform business expectations for Headspace. This effort is in support of Headspace’s growing efforts in leveraging data science to become more competitive in the wellness industry.,This Statement of Work (SOW) defines work to be performed by Harvard Data Analytics Group for Headspace through their Data Analytics and Business Intelligence team. This work consists of creating forecasting models on various metrics to help inform business expectations for Headspace. This effort is in support of Headspace’s growing efforts in leveraging data science to become more competitive in the wellness industry.,"Outcome: HDAG will build weekly or monthly forecasting models that forecast Headspace subscription metrics through December 2022, and generate a set of insights from the forecast models, culminating in a presentation on these key insights.","Python, R","**Tech Stack**: Python, R","free trial starts, free trial conversion rate, paid member renewal rate, average revenue per member, content starts, active days"
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F20_UNWTO_Case.pdf,F20,Case Name: UNWTO,UNWTO,"The project’s focus will be data analytics that can support tourism forecasting. Using Google trends data, UNWTO monthly and quarterly data on international tourist arrivals and international tourism receipts go correlate international tourist arrivals and spending per country.","The World Tourism Organization is the United Nations specialized agency responsible for the promotion of responsible, sustainable and universally accessible tourism. With the department of Market Intelligence & Competitiveness, the project’s focus will be data analytics that can support tourism forecasting. Using Google trends data, UNWTO monthly and quarterly data on international tourist arrivals and international tourism receipts go correlate international tourist arrivals and spending per country.",Outcome: The project’s focus will be data analytics that can support tourism forecasting.,**Programming Languages Used**: None,Tech Stack: Google trends data,"**KPIs**: international tourist arrivals, international tourism receipts, spending per country"
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F21_HP_Case.pdf,F21,HP,HP,HP is aiming to transition towards offering digital products by analyzing existing data from users of their devices to identify user pain points and new product offerings. The HDAG team will explore user data from HP’s devices and identify avenues for creating new features and services in the digital space.,"HP has traditionally been primarily a hardware company, with products such as printers, scanners and laptops. However, they are aiming to transition towards offering digital products. Towards that end, they want to analyze existing data from users of their devices in order to identify user pain points and new product offerings. For example, if they identified that users were commonly using scanners to scan signed documents, they could think about developing a service similar to docusign. The HDAG team will explore a large quantity of user data from HP’s devices, printers, scanners, etc. HP wants to get a grasp on the data and identify what data sources might be valuable for additional analysis. The HDAG team will identify avenues where HP can leverage its existing user base and product offerings to create new features and services primarily in the digital space. These insights can be based on market research as well as driven by the team’s exploration of the available data.","Outcome: The HDAG team will explore user data to identify pain points and new product offerings, and develop a digital product strategy leveraging existing user base and product offerings.",Programming Languages Used: None specified,"Data Analysis, Machine Learning",**KPIs**: Not explicitly mentioned in the document.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F21_Lightspeed_Case.pdf,F21,Lightspeed Venture Partners,Lightspeed Venture Partners,Automating data transformation to match LSVP’s existing data channels and may include building and deploying a machine learning model on the clients data on startups or founders.,Automating data transformation to match LSVP’s existing data channels and may include building and deploying a machine learning model on the clients data on startups or founders. This effort is in support of LSVP’s efforts in leveraging data science to become more competitive in the venture industry.,The document does not explicitly state an outcome.,Python,"Python, JSON, CSV, Machine Learning",**KPIs**: Not specified in the document.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F21_Onna_Case.pdf,F21,**Case Name**: Onna,Onna,"Onna would like a program to create a graph/network representing their employee workspace. The problem itself is very open-ended, and Onna is flexible with the delivery and construction of the project. Potential data sources include, slack messages, google drive interactions, email and confluence. The aim is to explore how employees collaborate using various tools and identify ways in which Onna’s tools can help.","Onna would like a program to create a graph/network representing their employee workspace. The problem itself is very open-ended, and Onna is flexible with the delivery and construction of the project. Potential data sources include, slack messages, google drive interactions, email and confluence. The aim is to explore how employees collaborate using various tools and identify ways in which Onna’s tools can help. Perform exploratory analysis creating a graph of employee workspace interactions, ideally packaged in a docker container. Access data using a secure environment or API. Skills: Data Analysis, Machine Learning, Graphs.","Outcome: Onna would like a program to create a graph/network representing their employee workspace, exploring how employees collaborate using various tools and identifying ways in which Onna’s tools can help.",Programming Languages Used: Not specified in the document.,"Tech Stack: Docker, Data Analysis, Machine Learning, Graphs",**KPIs**: Not provided in the document.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F21_RapidsYouthSoccer_Case.pdf,F21,Colorado Rapids Youth Soccer Club,Colorado Rapids Youth Soccer Club,"This project consists of using cost-benefits analysis to evaluate Colorado Rapids Youth Soccer Club’s social return on investment, quantifying the club’s economic, health, and social impact. This effort is in support of their efforts to leverage data science to fulfill their potential for growth, both on and off the field.","This project consists of using cost-benefits analysis to evaluate Colorado Rapids Youth Soccer Club’s social return on investment, quantifying the club’s economic, health, and social impact. This effort is in support of their efforts to leverage data science to fulfill their potential for growth, both on and off the field.",The document does not explicitly state an outcome.,Programming Languages Used: None specified,"Data Analysis, Machine Learning","**KPIs**: total annual social return on investment in local communities across economy, society, and health sectors."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F21_RedHat_Case.pdf,F21,Red Hat Enterprise Neurosystem,Red Hat,This Statement of Work (SOW) defines work to be performed by Harvard Data Analytics Group for Red Hat Enterprise Neurosystem. This work consists of training models across various hyperparameters and accelerators to compare which models fit well for a specific hardware. This effort is in support of Red Hat’s open source efforts in helping companies be competitive by creating a centralized AI framework that can be autonomously managed.,This Statement of Work (SOW) defines work to be performed by Harvard Data Analytics Group for Red Hat Enterprise Neurosystem. This work consists of training models across various hyperparameters and accelerators to compare which models fit well for a specific hardware. This effort is in support of Red Hat’s open source efforts in helping companies be competitive by creating a centralized AI framework that can be autonomously managed.,The document does not explicitly state an outcome.,"Python, GPU programming","CookieNet, GPU, FPGA",**KPIs**: Time to train on various hyperparameters on different kinds of accelerators.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F21_ThinkCERCA_Case.pdf,F21,ThinkCERCA A and B,ThinkCERCA,"ThinkCERCA created a patented technology designed to enable a research-based approach to improving literacy development on a large scale. The technology uses a six-step process that helps students break the task of close reading, critical thinking, and academic writing into smaller cognitive chunks, resulting in a formal piece of writing. It focuses on argumentative writing, which is crucial for college and career readiness, enabling students to achieve 2 years of reading growth per year and 20% growth in writing in as little as 8 weeks. The interactive argument-builder, or CERCA framework, allows students to input and organize argument components. In 2021, ThinkCERCA collaborated with experts to enable machine learning that draws on decades of student essays and teacher feedback, developing a reliable way to detect argument components using a fine-tuned model built with BERT.","ThinkCERCA created a patented technology designed to enable a research-based approach to improving literacy development on a large scale. Using a six-step process that helps students break the task of close reading, critical thinking, and academic writing across disciplines into smaller cognitive chunks, the technology walks students through a rigorous process resulting in a formal piece of writing. One of the key types of writing that ThinkCERCA focuses on is argumentative writing. This genre works across grade levels and subject areas, and it is the most powerful type of writing in terms of college and career readiness. Not only does ThinkCERCA help students improve their argumentative writing as proof of their learning, it also improves learning itself. It has been proven to enable students to achieve 2 years of reading growth per year and gain 20% growth in writing in as little as 8 weeks. One of the key features of this process is the interactive argument-builder, or CERCA framework interaction, where students input argument components which often end up being part of their final piece. They can then organize those pieces in the argument-builder step or later as they are developing final drafts. The CERCA framework has the following components: Claim, Evidence, Reasoning, Counterargument, Audience. Over the course of 2021, ThinkCERCA has been working with a variety of experts, learning scientists, data scientists, and engineers to enable machine learning that draws on the decades of student essays and teacher feedback captured in this technology. One of these partnerships included collaborating with HDAG throughout the spring (with 2 case teams working on automated student essay and teacher feedback) and summer. Through a blend of inputs coming from the content woven into the process by ThinkCERCA’s editorial team (prompts, texts, sentence stems, etc.,) student inputs in the six-step process, including the interactive, color-coded text boxes of the CERCA framework, and other sources of NLP, ThinkCERCA’s team has developed a relatively reliable way to detect the components of the arguments students write. The current model for identifying argument components is a fine-tuned model built with BERT, a powerful NLP pre-training tool developed by Google. We have even done some testing on other texts outside of our own student essays.",ThinkCERCA has been proven to enable students to achieve 2 years of reading growth per year and gain 20% growth in writing in as little as 8 weeks.,**Programming Languages Used**: BERT,BERT,2 years of reading growth per year; 20% growth in writing in as little as 8 weeks.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F22_Compassion_Case.pdf,F22,**Case Name**: Compassionchildren from poverty,Compassion International,"Compassion International is an American child sponsorship and humanitarian aid organization headquartered in Colorado Springs, Colorado, that aims to positively influence the long-term development of children globally who live in poverty.","Compassion supports specific partner churches or communities through Complementary Interventions (CIVs). While CIVs are well-documented, there is a gap in the categorization available for analytics. Existing CIV categories are often broad, with the specific activity associated with that intervention buried in textual descriptions. HDAG will develop an NLP categorization approach that (1) identifies appropriate groupings and (2) categorizes CIVs into these groupings. As a secondary activity, categorized CIVs could be correlated to program outcomes data in order to measure the impact of specific CIV types.","Compassion International is an American child sponsorship and humanitarian aid organization headquartered in Colorado Springs, Colorado, that aims to positively influence the long-term development of children globally who live in poverty.",Python,**Tech Stack**: Python,"**KPIs**: Group Extraction, NLP categorization, Outcomes Correlation analysis"
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F22_FemTech_Case.pdf,F22,**Case Name**: FemTech Focus,FemTech Focus,The HDAG team will work on developing scalable visualizations from FemTech datasets for integration into client facing interface.,"The HDAG team will work on developing scalable visualizations from FemTech datasets for integration into client facing interface. Exploratory Research. Use customer input (Zoom interviews) and comparable market example (e.g. Gartner, Forrester, Crunchbase, etc.) to determine useful visualizations and insights. Scalable Data Visualization. Write scalable code (using D3.js) to create visualizations and insights from FemTech data (startups database and exits database). UX Integration. Work with FemHealth’s UX designer to integrate code into website (current landing page). Client Partners + Meeting Expectations: Executive Director of FemTech & CEO of UX designer firm; weekly meetings expected. Datasets: Startups database, Exits database (available as Airtables). Deliverables: Midpoint and final deliverable. Preferred Coding Languages: JavaScript (esp. D3.js). Specific Skills: Research: Conducting market research. Data Visualization: Creating useful visualizations (specifically using JavaScript and D3.js). Data Analytics: Exploring, processing and deriving valuable insights from data. Expected Technical Difficulty: Easy.",The HDAG team will work on developing scalable visualizations from FemTech datasets for integration into client facing interface.,JavaScript (esp. D3.js),JavaScript (esp. D3.js),**KPIs**: Midpoint and final deliverable
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F22_AMA_Case.pdf,F22,**Case Name**: American Medical Association (AMA),American Medical Association,"The HDAG team will use data collected from Twitters Historical Powertrack API programmatically to classify Twitter users as Physicians, Residents & Fellows, Med Students, Health Care Providers/Other Clinicians (Medical Coders, Nurses, PA’s,Etc.). This could be accomplished using a Twitter users Bio and a sample of their most recent Tweets. AMA currently has a semi-robust method to identify Physicians and their specialty using string matching coupled with a filtering methodology but would like to improve upon this process and automate data ingestion/classification. Our team would build on this to remove junk contributors (users creating content for contents sake) and identify key influencers using data metrics. Our team will classify an existing list of Twitter users and develop a data pipeline to ingest new Twitter users and classify them as well.","The HDAG team will use data collected from Twitters Historical Powertrack API programmatically to classify Twitter users as Physicians, Residents & Fellows, Med Students, Health Care Providers/Other Clinicians (Medical Coders, Nurses, PA’s,Etc.). This could be accomplished using a Twitter users Bio and a sample of their most recent Tweets. AMA currently has a semi-robust method to identify Physicians and their specialty using string matching coupled with a filtering methodology but would like to improve upon this process and automate data ingestion/classification. Our team would build on this to remove junk contributors (users creating content for contents sake) and identify key influencers using data metrics. Our team will classify an existing list of Twitter users and develop a data pipeline to ingest new Twitter users and classify them as well.","The HDAG team will use data collected from Twitters Historical Powertrack API programmatically to classify Twitter users as Physicians, Residents & Fellows, Med Students, Health Care Providers/Other Clinicians (Medical Coders, Nurses, PA’s,Etc.). This could be accomplished using a Twitter users Bio and a sample of their most recent Tweets. AMA currently has a semi-robust method to identify Physicians and their specialty using string matching coupled with a filtering methodology but would like to improve upon this process and automate data ingestion/classification. Our team would build on this to remove junk contributors (users creating content for contents sake) and identify key influencers using data metrics. Our team will classify an existing list of Twitter users and develop a data pipeline to ingest new Twitter users and classify them as well.","R (w/ Tidyverse), Python, SQL","**Tech Stack**: R (w/ Tidyverse), Python, AWS (optional), SQL (optional)",**KPIs**: Not explicitly mentioned in the document.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F22_Moderna_Case.pdf,F22,**Case Name**: Maverick,Moderna,"The goal of this project is to develop an ML use case (project code name: Maverick) based on User Data located in ServiceNow, which is a help desk service to manage tickets & satisfaction and organize onboarding / offboarding tasks, and OKTA, a device & software authentication service. The ultimate goal of this engagement is to help accelerate the digitalization of Moderna. There are a few potential use cases outlined within the following verticals: “Perceive”, “Predict”, “Recommend”, and “Categorize”. After spending some time evaluating and discussing the feasibility of each use case, the team will choose the use case(s) which have the most value to Moderna. A rough outline of the project may look like: Presentations detailing the specific use cases which have the highest value proposition and the findings/insights generated from the baseline models/scores developed; Technical deliverables, including code repository, curated data sets, and model-based findings. The case team will report on the scalability of the use case and methods by which it can be deployed onto a larger scale.","The goal of this project is to develop an ML use case (project code name: Maverick) based on User Data located in ServiceNow, which is a help desk service to manage tickets & satisfaction and organize onboarding / offboarding tasks, and OKTA, a device & software authentication service. The ultimate goal of this engagement is to help accelerate the digitalization of Moderna. There are a few potential use cases outlined within the following verticals: “Perceive”, “Predict”, “Recommend”, and “Categorize”. After spending some time evaluating and discussing the feasibility of each use case, the team will choose the use case(s) which have the most value to Moderna. A rough outline of the project may look like: Presentations detailing the specific use cases which have the highest value proposition and the findings/insights generated from the baseline models/scores developed; Technical deliverables, including code repository, curated data sets, and model-based findings. The case team will report on the scalability of the use case and methods by which it can be deployed onto a larger scale.",The ultimate goal of this engagement is to help accelerate the digitalization of Moderna.,Python,Python,"**KPIs**: Presentations detailing the specific use cases which have the highest value proposition and the findings/insights generated from the baseline models/scores developed; Technical deliverables, including code repository, curated data sets, and model-based findings; Report on the scalability of the use case and methods by which it can be deployed onto a larger scale."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F22_Invitae_Case.pdf,F22,**Case Name**: Invitae,Invitae,"Invitae is a leader in medical genetic testing, driving genetic information into mainstream medicine to improve healthcare for billions of people. The project involves developing an algorithm to analyze claims data for reimbursement issues due to new requirements from payers, which are challenging to keep up with. The project consists of three main stages: consolidating and cleaning claims data, developing machine learning algorithms to detect anomalies in reimbursement, and implementing a system to notify relevant leaders of these changes.","Invitae performs a wide range of genetic tests, and reimbursement from 3rd-party commercial and government payers (such as Aetna, UnitedHealthcare, Medicare, etc.) is a critical source of revenue. However, the rapid growth of the genetic testing industry has prompted these payers to reexamine how they pay for these services, and they are imposing a diversity of new requirements for reimbursement such as new prior authorizations, additional documentation, evolving clinical criteria, and different coding rules. Due to the wide range of payers Invitae works with, these changes are difficult to keep up with, and significant time can elapse before the Invitae Billing & Reimbursement team is fully aware of a payment disruption and can take action to address the issue. Invitae believes there is an opportunity to develop an algorithm that will use claims data to surface these issues more quickly and comprehensively than they currently do today. Broadly, the project HDAG will engage with Invitae would consist of three main stages. 1. The HDAG team will first consolidate and clean claims data as needed, ensuring visibility into per-payer, per-product reimbursement performance. 2. After this is done, the HDAG team will develop machine learning algorithms that will highlight anomalies in how payers are reimbursing the services Invitae provides. 3. Once the anomaly detection algorithm is finalized, a system will be implemented to highlight these changes to the appropriate leaders in the organization.",The project HDAG will engage with Invitae to develop an algorithm that uses claims data to surface reimbursement issues more quickly and comprehensively.,"Python, SQL, Tableau","Python, SQL, Tableau","**KPIs**: Reimbursement performance per payer and per product, anomaly detection in reimbursement processes."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F22_ThermoFisher_Case.pdf,F22,ThermoFisher,Thermo Fisher Scientific Inc,"ThermoFisher wants HDAG to develop tools to analyze their competitors. They want to understand who their competitors are hiring, where they are advertising, how they are marketing themselves, and how their product portfolios are changing. The project will have three phases: Initial scraping to gather relevant information, specification and deep data collection on competitor companies, and analysis of the dataset to determine features that correlate with company success or imply new product introductions, culminating in a qualitative and quantitative report.","ThermoFisher wants HDAG to develop tools to analyze their competitors. They want to understand who their competitors are hiring, where they are advertising, how they are marketing themselves, and how their product portfolios are changing. To accomplish this goal, the project will have three phases: 1. Initial scraping. ThermoFisher wants us to initially look at a broad range of questions, and recognizes that some of these questions are more or less feasible to answer given the limitations of what data is accessible. This initial step will seek to gather as much relevant information as possible in order to understand which of their questions are answerable given the information we can access. 2. Specification and deep data collection. Once we understand what data is accessible and which companies the most data is available for, we will launch a deep dive into these competitor companies and collect all the data that we can on them. 3. Analysis. Once we have the desired dataset, we will analyze the data to determine which features correlate with company success or imply the introduction of new products, as well as differences between different competitors. Then, we will produce a qualitative and quantitative report on our findings and present it to ThermoFisher.","ThermoFisher wants HDAG to develop tools to analyze their competitors. They want to understand who their competitors are hiring, where they are advertising, how they are marketing themselves, and how their product portfolios are changing. The project will have three phases: Initial scraping, Specification and deep data collection, and Analysis.",Python,"**Tech Stack**: Python, NLP, web scraping tools, Machine Learning",TBD
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F23_CocaCola_Case.pdf,F23,Coca-Cola,Coca-Cola,"The objective of this case is to conduct a thorough analysis of the market fit for Coca-Cola by leveraging both internal company data and external market information. This analysis will encompass various aspects including brand performance, distribution channels, engagement models, competitive landscape, and financial performance. The key focus is to identify strengths and weaknesses and to provide valuable insights for informed decision-making.","The objective of this case is to conduct a thorough analysis of the market fit for Coca-Cola by leveraging both internal company data and external market information. This analysis will encompass various aspects including brand performance, distribution channels, engagement models, competitive landscape, and financial performance. The key focus is to identify strengths and weaknesses and to provide valuable insights for informed decision-making.","The outcome of the project is to utilize predictive analysis techniques, including machine learning, to create an interactive dashboard that provides data visualizations displaying key performance metrics with user-tweaked input values to simulate possible future outcomes.","Python/Excel, Tableau","Python/Excel, Tableau (optional)","- Industry market research and analyze brand performance to assess market recognition
- Assess the revenue generated to identify areas of improvement and optimization"
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F23_CAC_Case.pdf,F23,**Case Name**: Coaching Association of Canada,Coaching Association of Canada,"The main goal is to support the CAC in a holistic evaluation of its current E-Learning modules available for its coaches and to help them design a survey to better understand coaching demographics for the upcoming year. More specifically, the project will: conduct a thorough demographic analysis of current E-Learning users based on existing datasets, comparing findings with previous reports; analyze and suggest improvements to the User Experience (UX) and accessibility of the E-Learning platform; review past surveys of coaches to generate actionable recommendations for the upcoming survey of coaches in early 2024 that can help the CAC maximize response rate and learn more about its members.","The main goal is to support the CAC in a holistic evaluation of its current E-Learning modules available for its coaches and to help them design a survey to better understand coaching demographics for the upcoming year. More specifically, the project will:

●     Conduct a thorough demographic analysis of current E-Learning users based on existing datasets,
comparing findings with previous reports
●     Analyze and suggest improvements to the User Experience (UX) and accessibility of the E-Learning
platform
●     Review past surveys of coaches to generate actionable recommendations for the upcoming survey
of coaches in early 2024 that can help the CAC maximize response rate and learn more about its
members","The main goal is to support the CAC in a holistic evaluation of its current E-Learning modules available for its coaches and to help them design a survey to better understand coaching demographics for the upcoming year. More specifically, the project will:

●     Conduct a thorough demographic analysis of current E-Learning users based on existing datasets,
comparing findings with previous reports
●     Analyze and suggest improvements to the User Experience (UX) and accessibility of the E-Learning
platform
●     Review past surveys of coaches to generate actionable recommendations for the upcoming survey
of coaches in early 2024 that can help the CAC maximize response rate and learn more about its
members",Python,Tech Stack: Python,"**KPIs**: Conduct a thorough demographic analysis of current E-Learning users, analyze and suggest improvements to the User Experience (UX) and accessibility of the E-Learning platform, review past surveys of coaches to generate actionable recommendations for the upcoming survey of coaches in early 2024."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F23_NAACP_Case.pdf,F23,Brown v. Board of Education,NAACP Legal Defense Fund,"The goal of this project is to develop an application for monitoring and reporting suspected cases of election-related misinformation in real-time. Channels to monitor include, but are not limited to, elected officials’ websites, news articles, and Twitter. The application will be fed live data from predetermined sources, analyze and flag suspicious and anomalous content, and produce an alert/notification for each incident and or a recurring report summarizing new incidents.","The goal of this project is to develop an application for monitoring and reporting suspected cases of election-related misinformation in real-time. Channels to monitor include, but are not limited to, elected officials’ websites, news articles, and Twitter. The application will be fed live data from predetermined sources, analyze and flag suspicious and anomalous content, and produce an alert/notification for each incident and or a recurring report summarizing new incidents.",The goal of this project is to develop an application for monitoring and reporting suspected cases of election-related misinformation in real-time.,Python,Python,**KPIs**: Monitor and report suspected cases of election-related misinformation in real-time; analyze and flag suspicious content; produce alerts/notifications for incidents; generate recurring reports summarizing new incidents; maximize accuracy and generalizability of the truthfulness prediction model.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F23_ScanGlobal_Case.pdf,F23,Case Name: Scan Global Logistics,Scan Global Logistics,"This case will work with the financial team at Scan Global to optimize their account payable processes, particularly focusing on the efficient matching of bills. Although ideally automatic, the AP system often necessitates manual intervention to match incoming vendor invoices with records ""recapped"" by the operations team, leading to payment delays. Scan Global would like to better understand the differences in efficiency between different ingestion methods (EDI, OCR, and manual entry), what factors are causing matching issues (vendor #, invoice #, invoice amount, ref #, etc), and which vendors are causing the most delays (in time and number of touches).","This case will work with the financial team at Scan Global to optimize their account payable processes, particularly focusing on the efficient matching of bills. Although ideally automatic, the AP system often necessitates manual intervention to match incoming vendor invoices with records ""recapped"" by the operations team, leading to payment delays. Scan Global would like to better understand the differences in efficiency between different ingestion methods (EDI, OCR, and manual entry), what factors are causing matching issues (vendor #, invoice #, invoice amount, ref #, etc), and which vendors are causing the most delays (in time and number of touches).

HDAG will use historical data provided by the client detailing invoice processing, specifically who entered fields, who was the last touch on an invoice, and what was the timestamp for each action. By leveraging statistical analysis, machine learning modeling, and historical data, HDAG aims to furnish Scan Global with actionable insights to streamline and enhance their invoice processing system.","Scan Global Logistics is a global full-service logistics provider with its headquarters in Denmark. Established in 1975, the company operates across all six continents, positioning itself as a leader in the logistics and freight forwarding sector. They offer a broad spectrum of services, including air and sea freight, road transport, warehousing, and specialized logistics solutions, catering particularly to sectors like energy, automotive, and humanitarian logistics. Their approach emphasizes flexibility, speed, and a customer-centric viewpoint, aiming to simplify complex logistics challenges for their clients. The company boasts an annual revenue of approximately $3 billion USD.",Python,Python,"**KPIs**: Efficiency of account payable processes, matching accuracy of vendor invoices, payment delays, differences in efficiency between ingestion methods (EDI, OCR, manual entry), factors causing matching issues (vendor #, invoice #, invoice amount, ref #), vendors causing the most delays (in time and number of touches)."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F22_Roche_Case.pdf,F22,Case Name: HDAG,Roche US,"The goal of this project is to examine trends in Roche instrument repair case notes, using natural language processing and unsupervised machine learning methods. Broadly, the project HDAG will engage with Roche would consist of two main stages.","The goal of this project is to examine trends in Roche instrument repair case notes, using natural language processing and unsupervised machine learning methods. Broadly, the project HDAG will engage with Roche would consist of two main stages. 1) Feature Extraction: The HDAG team will first extract useful features from the raw free-form text data. This will be done using natural language processing (NLP) methods, including sentiment analysis using language models, topic modeling with methods such as Latent Dirichlet allocation, keyword extraction using statistical methods and machine learning, and/or other related NLP methodologies. 2) ML Modeling/Statistical Analysis: The HDAG team will employ statistical methods, in particular clustering techniques to perform unsupervised text categorization and identify trends in the instrument repair case notes data.","The goal of this project is to examine trends in Roche instrument repair case notes, using natural language processing and unsupervised machine learning methods.",Python,Python,**KPIs**: Not provided in the document.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F23_SkyGrid_Case.pdf,F23,Skygrid/Boeing,"SkyGrid, a Boeing SparkCognition Company","SkyGrid, a Boeing SparkCognition Company, is building an advanced airspace integration system, “Jupiter,” that will safely integrate remotely piloted aircraft into civilian airspace. The system is being built to develop functions with the adequate level of safety to enable operational approval of remotely piloted aircraft by domestic and international regulators. The goal of this project is to accurately model and estimate the future global market size for Jupiter, SkyGrid’s UAV airspace integration system, and develop a simulation model for scenario-testing which dynamically forecasts future market conditions depending on user-adjustable input variables and assumptions.","The goal of this project is to accurately model and estimate the future global market size for Jupiter, SkyGrid’s UAV airspace integration system, and develop a simulation model for scenario-testing which dynamically forecasts future market conditions depending on user-adjustable input variables and assumptions. Two case teams will be assigned to this case.

● Estimate and validate future product market size through a bottom-up approach. Conduct thorough market research and studies on the business landscape, followed by quantitative estimation of the market using a combination of independently collected and SkyGrid provided datasets.
● Research and develop pricing models and recommendations for Jupiter, taking into account cost-savings and prior-work performed on forecasting the market.
● Develop a model which enables the client to model how combinations and perturbations regarding input variables and assumptions alter the estimated market conditions and economics.","The outcome of the project is to accurately model and estimate the future global market size for Jupiter, SkyGrid’s UAV airspace integration system, and develop a simulation model for scenario-testing which dynamically forecasts future market conditions depending on user-adjustable input variables and assumptions.","Python, Excel.","Python, Excel",**KPIs**: Estimate and validate future product market size through a bottom-up approach; Research and develop pricing models and recommendations for Jupiter; Develop a model to analyze how input variables and assumptions alter estimated market conditions and economics.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F23_WooSox_Case.pdf,F23,Worcester Red Sox,Worcester Red Sox,"The main goal is to find solutions to bring as many fans as possible in Central MA and New England Region to the park especially for the weekday games (Tue - Thu). To do so, the project will explore and find insights on the current demographic, with specific goals including: Understand the demographic of not limited to central MA but surrounding New England region, in terms of market behavior, driving factors, etc. Explore the traffic of the website and other social media accounts to get more insights to run advertisements more efficiently. Find out how outside events (competitions) such as concerts, other sporting events, and other entertainments in the area could affect the ticket sales. Discover potential target markets in the area and how to attract their attention.","The main goal is to find solutions to bring as many fans as possible in Central MA and New England Region to the park especially for the weekday games (Tue - Thu). To do so, the project will explore and find insights on the current demographic, with specific goals including:

●    Understand the demographic of not limited to central MA but surrounding New England region, in
terms of market behavior, driving factors, etc.
●    Explore the traffic of the website and other social media accounts to get more insights to run
advertisements more efficiently.
●    Find out how outside events (competitions) such as concerts, other sporting events, and other
entertainments in the area could affect the ticket sales.
●    Discover potential target markets in the area and how to attract their attention.",The main goal is to find solutions to bring as many fans as possible in Central MA and New England Region to the park especially for the weekday games (Tue - Thu).,Python/SQL/Excel,Python/SQL/Excel,"Understand the demographic of not limited to central MA but surrounding New England region, in terms of market behavior, driving factors, etc. Explore the traffic of the website and other social media accounts to get more insights to run advertisements more efficiently. Find out how outside events (competitions) such as concerts, other sporting events, and other entertainments in the area could affect the ticket sales. Discover potential target markets in the area and how to attract their attention."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F22_SouthPole_Case.pdf,F22,South Pole,South Pole,"South Pole is a Swiss carbon finance consultancy founded in 2006 in Zurich, Switzerland. South Pole's business covers project and technology finance, data and advisory on sustainability risks and opportunities, as well as the development of environmental commodities such as carbon and renewable energy credits. This project will have three parts. South Pole is mostly looking to evaluate distributed ledger technology as a way to help increase the transparency and trustworthiness of the carbon markets. This case will be different from HDAG cases in the past since we’ll also be working with a team from Harvard Blockchain Club with more blockchain-specific experience. The HDAG and HBC team will attempt to tackle all three of the goals listed below over the course of the semester: 1. Make a first assessment in what ways DLT can provide that additional quality control and what is required to make that happen. 2. Explore a couple of DLT protocols and see which one fits our requirements best 3. Pick one protocol and develop a proof of concept for one or more project types (forestry, agriculture or renewable energy) focussing on the origination, verification and potential fractionalisation of the asset.","This project will have three parts. South Pole is mostly looking to evaluate distributed ledger technology as a way to help increase the transparency and trustworthiness of the carbon markets. This case will be different from HDAG cases in the past since we’ll also be working with a team from Harvard Blockchain Club with more blockchain-specific experience. The HDAG and HBC team will attempt to tackle all three of the goals listed below over the course of the semester:

1. Make a first assessment in what ways DLT can provide that additional quality control and what is required to make that happen.
2. Explore a couple of DLT protocols and see which one fits our requirements best
3. Pick one protocol and develop a proof of concept for one or more project types (forestry, agriculture or renewable energy) focussing on the origination, verification and potential fractionalisation of the asset.","South Pole is a Swiss carbon finance consultancy founded in 2006 in Zurich, Switzerland. South Pole's business covers project and technology finance, data and advisory on sustainability risks and opportunities, as well as the development of environmental commodities such as carbon and renewable energy credits.","Python, Javascript","Python, Javascript, CHIA, TOUCAN, Polygon, NEAR, Ethereum, Binance chain, EWC","**KPIs**: 1. Make a first assessment in what ways DLT can provide that additional quality control and what is required to make that happen. 2. Explore a couple of DLT protocols and see which one fits our requirements best. 3. Pick one protocol and develop a proof of concept for one or more project types (forestry, agriculture or renewable energy) focusing on the origination, verification and potential fractionalisation of the asset."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F23_USAG_Case.pdf,F23,**Case Name**: U.S. Aviation Group,U.S. Aviation Group,"The goal of the project is for HDAG to draw insights from internal data to help USAG understand trends in the maintenance of USAGs aircraft through dashboarding visualizations. HDAG will use Natural Language Processing tools to attempt to classify the specific internal system of the aircraft that an issue relates to, and this and other data to create real-time dashboarding software for USAG to use. HDAG will also deploy machine learning modeling to develop a model capable of predicting maintenance and type of maintenance on USAG aircraft.","The goal of the project is for HDAG to draw insights from internal data to help USAG understand trends in the maintenance of USAGs aircraft through dashboarding visualizations. HDAG will use Natural Language Processing tools to attempt to classify the specific internal system of the aircraft that an issue relates to, and this and other data to create real-time dashboarding software for USAG to use. HDAG will also deploy machine learning modeling to develop a model capable of predicting maintenance and type of maintenance on USAG aircraft.","The goal of the project is for HDAG to draw insights from internal data to help USAG understand trends in the maintenance of USAGs aircraft through dashboarding visualizations. HDAG will use Natural Language Processing tools to attempt to classify the specific internal system of the aircraft that an issue relates to, and this and other data to create real-time dashboarding software for USAG to use. HDAG will also deploy machine learning modeling to develop a model capable of predicting maintenance and type of maintenance on USAG aircraft.","Python, Tableau, SQL","Python, Tableau, SQL","**KPIs**: Insights from internal data, trends in aircraft maintenance, real-time dashboarding software, predictive maintenance model."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F24_Foodlink_Case.pdf,F24,**Case Name**: Foodlink NY Data Analytics Project,Foodlink,"The primary goal of this project is to strategically assist the new data analytics team at Foodlink NY with visualizations that would better represent and encapsulate their operations. This entails creating Tableau visualizations that would illustrate the current member network and other metrics, including geographic locations of Foodlink members across 10-county service area, dollar value of products distributed to each member, and a Foodlink inventory dashboard illustrating current products available in inventory. Additionally, the project involves advising the growing data analytics team at Foodlink with suggestions for improving operational efficiency.","The primary goal of this project is to strategically assist the new data analytics team at Foodlink NY with visualizations that would better represent and encapsulate their operations. This entails creating Tableau visualizations that would illustrate the current member network and other metrics, including (but not limited to): geographic locations of Foodlink members across 10-county service area, dollar value of products distributed to each member, and Foodlink inventory dashboard illustrating current products available in inventory. Additionally, the project involves advising the growing data analytics team at Foodlink with suggestions for improving operational efficiency.",The primary goal of this project is to strategically assist the new data analytics team at Foodlink NY with visualizations that would better represent and encapsulate their operations.,"Excel, Tableau, Python","Excel, Tableau, Python","**KPIs**: 25M pounds of food per year, around 350 community partners"
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F23_QuartzBio_Case.pdf,F23,QuartzBio,QuartzBio,"Automation of reporting packages that are largely manual – Business Intelligence and Machine Learning. Support QuartzBio’s creation of data visualization tools and dashboards using existing, defined metrics. Given a database of reports (stored in Excel and PPT), the goal is to develop clear, insightful, and actionable data visualization dashboards for Quartz to leverage in their other business workflows. Automations for end of month financial rollover, currently largely manual – Data Analytics. Each month, Quartz has ~80-100 Excel files (one per contract) that they need to port to Infor, an ERP tool. The goal is to develop automations for analyzing Excel data and porting it to Infor, as well as provide recommendations for managing the parallel process of having to update Excel files and have such changes reflected in Infor. After completion of the technical aspects of the project, Quartz also hopes to receive HDAG’s recommendations for operational analytics and other efficiencies based on the data we generate – Data Analytics and Consulting.","Automation of reporting packages that are largely manual – Business Intelligence and Machine Learning

Support QuartzBio’s creation of data visualization tools and dashboards using existing, defined metrics. Given a database of reports (stored in Excel and PPT), the goal is to develop clear, insightful, and actionable data visualization dashboards for Quartz to leverage in their other business workflows.

Automations for end of month financial rollover, currently largely manual – Data Analytics

Each month, Quartz has ~80-100 Excel files (one per contract) that they need to port to Infor, an ERP tool. The goal is to develop automations for analyzing Excel data and porting it to Infor, as well as provide recommendations for managing the parallel process of having to update Excel files and have such changes reflected in Infor.

After completion of the technical aspects of the project, Quartz also hopes to receive HDAG’s recommendations for operational analytics and other efficiencies based on the data we generate – Data Analytics and Consulting.","Outcome: QuartzBio aims to automate reporting packages and end-of-month financial rollovers, develop data visualization tools, and receive recommendations for operational analytics and efficiencies.","Excel, Python","Excel, Python","**KPIs**: Development of clear, insightful, and actionable data visualization dashboards; automation of end of month financial rollover; recommendations for operational analytics and efficiencies."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F24_Macys_Case.pdf,F24,Macy’s Star Rewards Credit Card Program,Macy’s,"The main goal of this project is to investigate Macy’s Star Rewards Credit Card Program and design a machine learning model to simulate consumer behavior after advertisement campaigns. This entails conducting exploratory analysis on Macy’s first-party data on cardholders and non-cardholders, investigating the behavioral trends of cardholders and non-cardholders, identifying who Macy’s should hyper-target to increase its credit card applications, designing a stand-alone model to simulate the impact of hyper-targeted advertisement on credit card applications using various machine learning models, and building out a data-pipeline for this stand-alone model if time permits.","The main goal of this project is to investigate Macy’s Star Rewards Credit Card Program and design a machine learning model to simulate consumer behavior after advertisement campaigns. This entails:

●    Conducting exploratory analysis on Macy’s first-party data on cardholders and non-cardholders.
●    Investigating the behavioral trends of cardholders and non-cardholders.
●    Identifying who Macy’s should hyper-target to increase its credit card applications.
●    Designing a stand-alone model to simulate the impact of hyper-targeted advertisement on credit
card applications using various machine learning models.
●    Building out a data-pipeline for this stand-alone model if time permits.",The main goal of this project is to investigate Macy’s Star Rewards Credit Card Program and design a machine learning model to simulate consumer behavior after advertisement campaigns.,Python; R and Stata,Python; R and Stata,**KPIs**: Not specified in the document.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F24_CZ_Case.pdf,F24,Chan Zuckerberg Initiative,Chan Zuckerberg Initiative,"The main goal of this project is to strategically assist the housing affordability team at the Chan-Zuckerberg Initiative, utilizing technical expertise to contribute towards their research agenda portfolio. This entails developing a machine learning tool to simulate various economic scenarios (e.g. inflation, mortgage rates, levels of gov spending, private investment levels) and their impacts on housing issues (new housing production, residential sales, housing costs, displacement rates of households, mortgage lending), incorporating predictions of housing prices based on historical data, and creating a dashboard that visualizes the percentage of households who can afford to buy a home in their city or region based on local incomes and housing costs.","The main goal of this project is to strategically assist the housing affordability team at the Chan-Zuckerberg Initiative, utilizing technical expertise to contribute towards their research agenda portfolio. This entails:

●     Developing a machine learning tool to simulate various economic scenarios (e.g. inflation, mortgage
rates, levels of gov spending, private investment levels) and their impacts on housing issues (new
housing production, residential sales, housing costs, displacement rates of households, mortgage
lending).
●     Incorporating predictions of housing prices based on historical data
●     Creating a dashboard that visualizes the percentage of households who can afford to buy a home in
their city or region based on local incomes and housing costs.","The main goal of this project is to strategically assist the housing affordability team at the Chan-Zuckerberg Initiative, utilizing technical expertise to contribute towards their research agenda portfolio. This entails developing a machine learning tool to simulate various economic scenarios and their impacts on housing issues, incorporating predictions of housing prices based on historical data, and creating a dashboard that visualizes the percentage of households who can afford to buy a home in their city or region based on local incomes and housing costs.","Python, Tableau","Python, Tableau","**KPIs**: Developing a machine learning tool to simulate various economic scenarios and their impacts on housing issues, incorporating predictions of housing prices based on historical data, creating a dashboard that visualizes the percentage of households who can afford to buy a home in their city or region based on local incomes and housing costs."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S21_FirstTake_Case.pdf,S21,**Case Name**: First Take,First Take,"First Take is a sports podcast that is part of the ESPN network. The First Take team will perform empirical data analysis on sports card markets and provide insights/analysis to Jeffrey Lachman (head of analytics at Chicago White Sox). More influentially, the First Take team will write a program to scrape images of sports cards from various online retailers, such as PSA, COMC or Ebay. This team will require analysts with Python/web scraping abilities and database knowledge (such as PostgreSQL). Goal: Scrape tens of thousands of images, store in database, provide insights on the industry as a whole. Skills: Web scraping, Data analytics, Visualization, Database Systems","First Take is a sports podcast that is part of the ESPN network. The First Take team will perform empirical data analysis on sports card markets and provide insights/analysis to Jeffrey Lachman (head of analytics at Chicago White Sox). More influentially, the First Take team will write a program to scrape images of sports cards from various online retailers, such as PSA, COMC or Ebay. This team will require analysts with Python/web scraping abilities and database knowledge (such as PostgreSQL). Goal: Scrape tens of thousands of images, store in database, provide insights on the industry as a whole. Skills: Web scraping, Data analytics, Visualization, Database Systems","Outcome: The First Take team will perform empirical data analysis on sports card markets, scrape images of sports cards from online retailers, and provide insights to Jeffrey Lachman. They require analysts with Python/web scraping abilities and database knowledge to achieve their goal of scraping tens of thousands of images, storing them in a database, and providing industry insights.","Python, PostgreSQL","Python, PostgreSQL","**KPIs**: Scrape tens of thousands of images, store in database, provide insights on the industry as a whole."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/F24_Odyssey_Case.pdf,F24,Odyssey Therapeia,Odyssey Therapeia,"Odyssey Therapeia is a biotechnology company founded in 2020 and based in Bengaluru, India. The company focuses on developing next-generation biotechnology products and services, with partnerships across Big Tech (Amazon, Oracle, etc.) as well as customers across India (integrated into all the major hospital networks across the country). The main goal is to transform Odyssey’s existing LLM-based patient management platform Concierge into a multitenant-capable full fledged AI SaaS platform for healthcare by focusing on the creation of an adapter system for heterogeneous healthcare data endpoints. This entails developing a universal data adapter that standardizes data formats across different healthcare systems, simplifying integration and reducing costs, building a robust data ingestion system to handle real-time healthcare data for improved decision-making and patient care, implementing advanced security measures to ensure data compliance with regulations such as HIPAA, along with strict data isolation for each healthcare provider, and integrating the above data ingestion mechanisms with existing and new infrastructure for harnessing autonomous LLM agents.","The main goal is to transform Odyssey’s existing LLM-based patient management platform Concierge into a multitenant-capable full fledged AI SaaS platform for healthcare by focusing on the creation of an adapter system for heterogeneous healthcare data endpoints. This entails:

●    Developing a universal data adapter that standardizes data formats across different healthcare
systems, simplifying integration and reducing costs.
●    Building a robust data ingestion system to handle real-time healthcare data for improved
decision-making and patient care.
●    Implementing advanced security measures to ensure data compliance with regulations such as
HIPAA, along with strict data isolation for each healthcare provider.
●    Integrating the above data ingestion mechanisms with existing and new infrastructure for harnessing
autonomous LLM agents.","Odyssey Therapeia is a biotechnology company founded in 2020 and based in Bengaluru, India. The company focuses on developing next-generation biotechnology products and services, with partnerships across Big Tech (Amazon, Oracle, etc.) as well as customers across India (integrated into all the major hospital networks across the country).",Python,Python,"**KPIs**: Developing a universal data adapter, building a robust data ingestion system, implementing advanced security measures, integrating data ingestion mechanisms with existing infrastructure."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S21_Schneider_Case.pdf,S21,**Case Name**: Schneider A (LASIK),Schneider Electric,"This project is a continuation of work done by HUDAG with Schneider last semester. The project scope falls into two parts: 1. IP (Indirect Procurement) Spend Forecasts IP Spends are purchases made to support the business and are important to ensure smooth operations. This project entails forecasting the company’s IP Spend across divisions. Beyond historical IP Spending trends, this project will also require creativity in ideating which datasets may be useful. Previously, public financial data was used but additional datasets can be explored. 2. Purchase Request Automation The purchase request process for Schneider Employees is currently a manual process that is prone to error since the metadata/categories marked often don’t match the item requested. This project will focus on creating a model to flag incorrect requests, potentially using Natural Language Processing techniques. Goal: Identify better ways to forecast IP spend, improve models used to flag incorrect purchase requests Skills: Data Analytics, Machine Learning, NLP","Detailed Case Description: This project is a continuation of work done by HUDAG with Schneider last semester. The project scope falls into two parts: 1. IP (Indirect Procurement) Spend Forecasts - This project entails forecasting the company’s IP Spend across divisions, requiring creativity in ideating which datasets may be useful. 2. Purchase Request Automation - This project will focus on creating a model to flag incorrect requests, potentially using Natural Language Processing techniques. Goal: Identify better ways to forecast IP spend, improve models used to flag incorrect purchase requests. Skills: Data Analytics, Machine Learning, NLP.","Outcome: Identify better ways to forecast IP spend, improve models used to flag incorrect purchase requests",**Programming Languages Used**: Not specified in the document.,"Data Analytics, Machine Learning, NLP","**KPIs**: Forecast accuracy of IP spend, reduction in incorrect purchase requests, efficiency of purchase request process."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S21_UnderArmour_Case.pdf,S21,UnderArmour,UnderArmour,"UnderArmour is a disruptive player in the sports apparel industry. This project will focus on using UA’s consumer data to answer two key questions related to their branding and customer experience: 1. Is Under Armour’s Purpose, Mission, Vision, and Values resonating with our consumers? Are they using the same words we use here or are they using different words? Which of these are most important or drive the greatest emotion/passion in our consumer? 2. We want to stand for Simplicity and Convenience from a consumer experience perspective across all of our channels. Based on survey and social data --- What does simplicity and convenience mean to our consumers? Where do we have opportunities across our channels to improve and differentiate in these areas? Data sources available include UA purchase channel surveys, UA product reviews, UA app reviews, UA social media mentions.","UnderArmour is a disruptive player in the sports apparel industry. This project will focus on using UA’s consumer data to answer two key questions related to their branding and customer experience: 1. Is Under Armour’s Purpose, Mission, Vision, and Values resonating with our consumers? Are they using the same words we use here or are they using different words? Which of these are most important or drive the greatest emotion/passion in our consumer? 2. We want to stand for Simplicity and Convenience from a consumer experience perspective across all of our channels. Based on survey and social data --- What does simplicity and convenience mean to our consumers? Where do we have opportunities across our channels to improve and differentiate in these areas? Data sources available include UA purchase channel surveys, UA product reviews, UA app reviews, UA social media mentions. Goal: Derive key insights from UnderArmour’s data sources. Skills: Data Analytics, Marketing Analytics, light NLP.",Outcome: Derive key insights from UnderArmour’s data sources.,**Programming Languages Used**: None specified.,"Data Analytics, Marketing Analytics, light NLP",**KPIs**: Not specified in the document.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S21_ThinkCERCA_Case.pdf,S21,ThinkCERCA,ThinkCERCA,"ThinkCERCA is a Chicago-based education company that creates K-12 writing software and curriculum to help students become strong critical thinkers. With roughly 30,000 schools involved, ThinkCERCA is a leading Edtech startup and is disrupting traditional education.","ThinkCERCA is a Chicago-based education company that creates K-12 writing software and curriculum to help students become strong critical thinkers. With roughly 30,000 schools involved, ThinkCERCA is a leading Edtech startup and is disrupting traditional education. ThinkCERCA Team A will perform data analysis on CERCA, ThinkCERCA’s flagship curriculum model, and accordingly provide insights that will further accelerate this model. This team will report to the CEO. The goal is to provide insights that will accelerate their curriculum and continue disrupting greater education. Skills required include Data Analytics, Visualization, and Machine Learning. ThinkCERCA Team B will focus on evaluating the heuristics used to evaluate student’s writing. This project will potentially involve NLP techniques to automatically extract key features of students writing samples. The goal is to build tools to help teacher’s evaluate student’s performance. Skills required include Data Analysis, Machine Learning, Visualization, and NLP.","ThinkCERCA A: Provide insights that will accelerate their curriculum and continue disrupting greater education.
ThinkCERCA B: Build tools to help teacher’s evaluate student’s performance.",**Programming Languages Used**: Not specified in the document.,"**Tech Stack**: Data Analytics, Visualization, Machine Learning, NLP",**KPIs**: Provide insights that will accelerate their curriculum and continue disrupting greater education; Build tools to help teacher’s evaluate student’s performance.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S21_UNIDO_Case.pdf,S21,Case Name: UNIDO Industrial Development and Sustainable Development Goals Project,UNIDO,Explore impact of industrial development indicators on progress towards sustainable development goals.,Explore impact of industrial development indicators on progress towards sustainable development goals.,Outcome: Explore impact of industrial development indicators on progress towards sustainable development goals.,**Programming Languages Used**: None specified.,"Tech Stack: Data Analysis, Regression Analysis, Visualization, Research",**KPIs**: Explore impact of industrial development indicators on progress towards sustainable development goals.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S22_ACE_Case.pdf,S22,Animal Charity Evaluators,Animal Charity Evaluators,"Animal Charity Evaluators (ACE) is an altruism-focused organization and a leading non-profit charity evaluator in the US. Their mission is to help animals, one method being increasing the impact of donations to animal charities, and have an estimated influence of over $30 million in the animal advocacy movement. They invest much into the research for top charities animal advocacy, publishing findings on their site (https://animalcharityevaluators.org/).","Animal Charity Evaluators (ACE) is an altruism-focused organization and a leading non-profit charity evaluator in the US. Their mission is to help animals, one method being increasing the impact of donations to animal charities, and have an estimated influence of over $30 million in the animal advocacy movement. They invest much into the research for top charities animal advocacy, publishing findings on their site (https://animalcharityevaluators.org/). This project will have two parts. Animal Charity Evaluators is looking to develop analyses over 1. Global funding flows for charities and 2. Trends in effects on animals. An HDAG team will tackle the goals sequentially, and the client is very understanding if some harder technological requests are out of reach (machine learning models, database integration to website). 1. Create a database for ACE that visualizes the compiled list of grant websites, identifying which charities are receiving funding, how much, and from whom. The first priority. 2. Create a similarly updating database that integrates multiple sources that measure animals affected. Measure and track variables such as population, weight, slaughter metrics, and such. Internal Partners: Head of Data Analytics Division for ACE. Datasets: Will be provided by ACE. For part 1, a compiled list of charity grant websites and information; for part 2, a compiled list of animal statistics websites. Preferred Coding Languages: Any, Python Specific Skills 1. Data Analytics: Identifying charity funding, sources, and visualizing such a network. Web scraping and automation are also helpful. 2. Database Creation: Integrating multiple sources of information into one pipeline for analysis and visualization. 3. Machine Learning: Second phase, developing a predictive model for animal statistics, ex. slaughter numbers, weight, and other factors. 4. Data Visualization and Interpretability: Creating useful and interactive visualizations to interpret the data. Communicating the limits of any model(s) to the client. Expected Technical Difficulty: Intermediate","Animal Charity Evaluators is looking to develop analyses over global funding flows for charities and trends in effects on animals. The project involves creating a database for ACE that visualizes grant websites, identifying which charities are receiving funding, how much, and from whom, as well as a database that integrates multiple sources measuring animals affected.",Python,**Tech Stack**: Python,"**KPIs**: 1. Create a database for ACE that visualizes the compiled list of grant websites, identifying which charities are receiving funding, how much, and from whom. 2. Create a similarly updating database that integrates multiple sources that measure animals affected. Measure and track variables such as population, weight, slaughter metrics, and such."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S22_Activision_Case.pdf,S22,**Case Name**: Activision,,"The HDAG team will work on a machine learning project for player clustering based on Activision’s user data within its Call of Duty mobile game. Currently, they have stable cluster analysis based on CoD franchise engagement, and Multiplayer mode preference/engagement. Other cohort definitions are mostly rule-based. In this project, they would like to explore cluster/segmentation analysis on more complex player behaviors, such as store content preference, engagement patterns, and spending patterns, etc. Successful outcome of this project will help Activision to: - Identify underserved player segments that we can focus our efforts on - Find more useful and stable segmentation to be included in our player profile, like device category clusters.","The HDAG team will work on a machine learning project for player clustering based on Activision’s user data within its Call of Duty mobile game. Currently, they have stable cluster analysis based on CoD franchise engagement, and Multiplayer mode preference/engagement. Other cohort definitions are mostly rule-based. In this project, they would like to explore cluster/segmentation analysis on more complex player behaviors, such as store content preference, engagement patterns, and spending patterns, etc. Successful outcome of this project will help Activision to: - Identify underserved player segments that we can focus our efforts on - Find more useful and stable segmentation to be included in our player profile, like device category clusters. Internal Partners: Director of Data Science and Product Analytics, Activision Mobile. Datasets: Activision-provided micro (player-level) user data, with variables associated with player experience. Data wrangling will be necessary for creating variables for in-game player experiences. Coding Languages: Python. Specific Skills 1. Data Management & Analytics: Wrangling, joining, reshaping data (management) and exploring, processing and deriving valuable insights from data (analytics) 2. Machine Learning: Using machine learning algorithms to gain insights into data. 3. Statistical Modeling: Perform data fitting analysis (Euclidean distance, k-means, spectral clustering, etc.) and model performance (goodness of fit, R^2, etc.) Expected Technical Difficulty: Intermediate / Advanced.","Successful outcome of this project will help Activision to:
-    Identify underserved player segments that we can focus our efforts on
-    Find more useful and stable segmentation to be included in our player profile, like device
category clusters",Python,Python,"**KPIs**: Identify underserved player segments, find more useful and stable segmentation to be included in player profile."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S22_Cinepolis_Case.pdf,S22,Cinepolis,Cinepolis,"The HDAG team will work to extract insights from Cinepolis’ promotional campaigns data (aka. challenges), working directly with the Cinepolis Head of Data Science and data science manager.","The HDAG team will work to extract insights from Cinepolis’ promotional campaigns data (aka. challenges), working directly with the Cinepolis Head of Data Science and data science manager. In particular, the client has expressed interest in the following questions and work. We note that some combination (rather than all) of these questions are likely to be explored over the semester. 1. Exploratory Data Analysis (EDA). - Customer analytics - Is there a particular profile of customers that frequently participate in challenges? That frequently accomplish challenges? How is the participation rate of the members changing over time? Is there a delayed effect of the challenges on customers? - Revenue analytics - What type of campaigns, customer targets, and length of campaigns seems to drive higher incremental revenue? - Factor analysis - What external factors play a role in campaign participation? (e.g. email subjects, movie content available, COVID cases, competitors, user’s geographic location, etc.) 2. Data Modeling and Visualization - Individual user model – what type of promotion will work best to maximize revenue given each individual user’s characteristics? - Holistic revenue model – what type of promotion will work best for the loyalty customer base as a whole? Internal Partners: Cinepolis Head of Data Science, Data Science Manager Datasets: Transactional, demographic, and tracking data for customers and specific campaigns. Coding Languages: Python (PySpark preferred but not necessary). Specific Skills 1. Data Analytics: Exploring, processing and deriving valuable insights from data 2. Data Visualization: Creating useful and interactive visualizations 3. Machine Learning: Training, testing, and deploying predictive models Expected Technical Difficulty: Intermediate","Cinepolis is a México-based multinational owner and operator of movie theatres. It is the world’s fourth largest movie theater circuit in the world, operating over 700 cinema complexes across 14 countries worldwide.",Python (PySpark preferred but not necessary),Python (PySpark preferred but not necessary),"**KPIs**: Customer participation rate, incremental revenue from campaigns, external factors affecting participation, effectiveness of promotions on individual users, overall revenue impact on loyalty customer base."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S22_CLF_Case.pdf,S22,Cambridge Local First,Cambridge Local First,"Cambridge Local First (CLF) is a non-profit network of 450+ local and independent businesses in Cambridge, Massachusetts. Their mission is to support, promote, and celebrate a “local economy community” by educating the public and government about the significant environmental, economic, and cultural benefits of a strong local economy.","The HDAG team will work on a set of 3 CLF strategic initiatives over the semester. 1. Data insights for State of Small Business Report. CLF is looking to better understand the welfare of small businesses in Cambridge and are drafting a State of Small Business Report and hope to use these data insights in the report itself, which will be shared with the public. 2. User engagement analysis. Review and generate visual insights (user engagement, etc.) from any/all of our three social media accounts (Instagram, Facebook, and Twitter) and our website (via Google Analytics and Jetpack) 3. Surveying businesses on the impacts of bicycle safety: CLF are looking to collect data from businesses on Mass Ave to assess the impact of protected bike lanes on business revenues. This would involve (1) creating a survey, (2) working to amplify the survey with affected businesses, and (3) generating insights from the survey.","Cambridge Local First (CLF) is a non-profit network of 450+ local and independent businesses in Cambridge, Massachusetts. Their mission is to support, promote, and celebrate a “local economy community” by educating the public and government about the significant environmental, economic, and cultural benefits of a strong local economy.",Any,Any,"**KPIs**: User engagement metrics from social media accounts, insights from the State of Small Business Report, survey results on the impact of bicycle safety on business revenues."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S22_Greystar_Case.pdf,S22,Greystar,Greystar,"The HDAG team will work to answer whether Greystar's data, combined with other publicly or commercially available data, can identify the most pertinent factors for rental pricing and provide actionable insights. Indeed, optimal rental pricing is foundational to Greystar's operational and financial success. The factors that drive rental prices are many and varied, and include obvious factors such as the broader economy, amenities, square footage, #bedrooms, #bathrooms, and location but may also include more subtle relationships. A rough outline of the project may look like: · Phase I of this project will identify candidate data and create a correlation matrix to determine the potential predictability of candidate variables. · Phase II of this project will involve a regression analysis or other applicable analysis to determine which, if any, candidate variables offer true predictive value and to what extent. · Phase III of this project will apply findings from Phase II to prescribe optimal pricing for each property. · Phase IV, if possible, will prescribe what enhancements could be applied to a specific property, or alternatively, what properties have the highest potential for enhancement given the pricing potential of that property's market factors.","The HDAG team will work to answer whether Greystar's data, combined with other publicly or commercially available data, can identify the most pertinent factors for rental pricing and provide actionable insights. Indeed, optimal rental pricing is foundational to Greystar's operational and financial success. The factors that drive rental prices are many and varied, and include obvious factors such as the broader economy, amenities, square footage, #bedrooms, #bathrooms, and location but may also include more subtle relationships. A rough outline of the project may look like: · Phase I of this project will identify candidate data and create a correlation matrix to determine the potential predictability of candidate variables. · Phase II of this project will involve a regression analysis or other applicable analysis to determine which, if any, candidate variables offer true predictive value and to what extent. · Phase III of this project will apply findings from Phase II to prescribe optimal pricing for each property. · Phase IV, if possible, will prescribe what enhancements could be applied to a specific property, or alternatively, what properties have the highest potential for enhancement given the pricing potential of that property's market factors.","The HDAG team will work to answer whether Greystar's data, combined with other publicly or commercially available data, can identify the most pertinent factors for rental pricing and provide actionable insights.","Python, SQL, R","Python, SQL, MSFT Office, R","**KPIs**: Identify the most pertinent factors for rental pricing, provide actionable insights, determine potential predictability of candidate variables, assess true predictive value of candidate variables, prescribe optimal pricing for each property, recommend enhancements for specific properties."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S22_HP_Case.pdf,S22,HP,HP Inc.,"HP is targeting an audience that prefers paper documents with the hope of helping them to slowly transfer physical documents to digital documents using HP products. This project is a continuation of HDAG's engagement from last semester where a case team worked to build an NLP model, using TF-IDF/raw text NLP featurization to try to predict whether a form requires a signature. The previous team has already extracted the most common words (unigrams) and pairs of words (bigrams) from a sample dataset and applied Tf-Idf vectorization to implement basic NLP models to detect forms that require a signature. The case for this semester would build upon this progress by performing two main tasks. The first task would be to expand the dataset of scanned forms so that the model has more data to work with, which would hopefully improve the accuracy of the NLP classifier. The second task would be to experiment with more classification techniques including bootstrapping and decision trees. The second part of the project has more freedom and would allow the case team to explore many different and complex ways of classifying documents.","HP is targeting an audience that prefers paper documents with the hope of helping them to slowly transfer physical documents to digital documents using HP products. This project is a continuation of HDAG's engagement from last semester where a case team worked to build an NLP model, using TF-IDF/raw text NLP featurization to try to predict whether a form requires a signature. The previous team has already extracted the most common words (unigrams) and pairs of words (bigrams) from a sample dataset and applied Tf-Idf vectorization to implement basic NLP models to detect forms that require a signature. The case for this semester would build upon this progress by performing two main tasks. The first task would be to expand the dataset of scanned forms so that the model has more data to work with, which would hopefully improve the accuracy of the NLP classifier. The second task would be to experiment with more classification techniques including bootstrapping and decision trees. The second part of the project has more freedom and would allow the case team to explore many different and complex ways of classifying documents.",The project aims to enhance an NLP model for predicting whether forms require signatures by expanding the dataset of scanned forms and experimenting with various classification techniques.,Python,"Python, Natural Language Processing, Pandas, Numpy, scikit-learn",**KPIs**: Improve the accuracy of the NLP classifier by expanding the dataset of scanned forms and experimenting with more classification techniques including bootstrapping and decision trees.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S21_IDEO_Case.pdf,S21,Case Name: HUDAG partnership,IDEO,,Analyze which types of messaging campaigns will best promote uptake of the COVID-19 vaccine,Outcome: Analyze which types of messaging campaigns will best promote uptake of the COVID-19 vaccine,**Programming Languages Used**: None specified.,"Tech Stack: Data Analytics, Marketing Analytics, Research",**KPIs**: Analyze which types of messaging campaigns will best promote uptake of the COVID-19 vaccine.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S22_Siemens_Case.pdf,S22,Siemens Energy,Siemens Energy,"Siemens Energy is looking to develop analytics that predict and identify power outages based on internet history and weather data. The project involves creating a database of Google searches related to power outages, developing a NLP/ML model to identify important keywords for predicting outages, and correlating weather data with outage history to create a predictive ML model.","Siemens Energy is mostly looking to develop analytics that predict and identify power outages based on internet history and weather data. An HDAG team will attempt to tackle all three goals over the course of the semester, but the client understands that potentially only the first two goals may be attainable in the given time frame:

1. Create a database of the Google (or other web engine) searches that could be emblematic of a power outage. Compare this to a known database of outages over the past couple of years.
2. Create a NLP/ML model that determines the most important keywords to predict an outage. This model should be able to determine the most relevant keywords by comparing the two databases that were created above.
3. Correlate weather data to outage history. Find a suitable weather dataset that is interpretable and powerful enough to predict outage history. Create a ML model that bridges the connection between these two data sources.","Siemens Energy is looking to develop analytics that predict and identify power outages based on internet history and weather data. The project has three goals: creating a database of Google searches related to power outages, developing an NLP/ML model to identify important keywords for predicting outages, and correlating weather data to outage history to create a predictive ML model.",Any,"**Tech Stack**: NLP, ML, Time series analysis, regressions, geospatial neural networks",**KPIs**: Not explicitly mentioned in the document.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S23_Bluebonnet_Case.pdf,S23,**Case Name**: Bluebonnet Data,Bluebonnet Data,"Bluebonnet Data is a nonprofit organization whose mission is to “democratize” data. They recruit, train, and organize people with skills in coding and data science to volunteer on teams for progressive campaigns and causes. The project HDAG will engage with Bluebonnet Data to invent a new matching algorithm that takes into account both ranking preferences and team composition (skill sets, time zones, etc.), implement this algorithm in Python, and utilize it to match real teams for Bluebonnet's next engagement cycle on April 20th.","Bluebonnet Data is a nonprofit organization whose mission is to “democratize” data. They recruit, train, and organize people with skills in coding and data science to volunteer on teams for progressive campaigns and causes. A pillar of the Bluebonnet Data organization is their ability to quickly match teams of data analysts to provide direct support to progressive campaigns and organizations. These matches are extremely important in ensuring a positive experience for both the analysts and the client, and must take into account both the analysts’ skill sets and interest in the subject. Currently, Bluebonnet has an algorithm that gives team match recommendations. However, it is not particularly robust. It requires a group to spend hours sifting through the automatically generated matches to take into account factors that the algorithm cannot. As the organization grows, this model is becoming less and less feasible. Bluebonnet believes that it is time to overhaul their recommendation algorithm completely, as well as determine a method for automatically adjusting the suggested matches. Broadly, the project HDAG will engage with Bluebonnet Data would consist of: 1. The HDAG team will work to invent a new matching algorithm that takes into account both ranking preferences and team composition (skill sets, time zones, etc.). 2. The team will implement this algorithm in Python. 3. The group will utilize this algorithm to match real teams for Bluebonnets next engagement cycle, on April 20th. Internal Partners: ● Jake Jackson (Technical Director) ● Mariam Ahmed (Operations Manager) ● John Attisha (Deputy Data Director) Coding Languages: Python Specific Skills 1. Algorithms: Creating matching algorithms using mathematical tools 2. Model Creation: Implementing a model that matches teams of analysts based on preferences and skill sets Expected Technical Difficulty: Intermediate","The outcome of the project is to invent a new matching algorithm that takes into account both ranking preferences and team composition, implement this algorithm in Python, and utilize it to match real teams for Bluebonnet's next engagement cycle on April 20th.",Python,Python,"**KPIs**: The project aims to develop a new matching algorithm that effectively considers ranking preferences and team composition, implement it in Python, and utilize it to match real teams for Bluebonnet's next engagement cycle on April 20th."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S22_Roche_Case.pdf,S22,**Case Name**: Immersive Patient Experience Project,Roche,"The Immersive Patient Experience Project is meant to improve the quality of life for volunteers who participate in clinical trials. The project aims to address the challenges faced by volunteers, such as taking time off work and dealing with additional effects, which often lead to dropouts or lack of participation. It seeks to bridge the gap in understanding between clinical trial designers and the volunteers' experiences by using a VR experience that simulates the patient journey. This will help healthcare providers empathize with volunteers and find solutions to improve clinical trial design. The project consists of two parts: providing recommendations for VR data sources and assisting in coding the VR experience while analyzing the data collected from the MVP.","The Immersive Patient Experience Project is meant to improve the quality of life for volunteers who participate in clinical trials. Often, it is difficult to source volunteers for a clinical trial because of the difficulty they must go through in order to take time out of their schedule to participate. Clinical trials often require volunteers to take time off from work, miss work days, and deal with the additional effects, causing many volunteers to drop out or not participate at all.

However, there is currently a gap in the understanding of the individuals who design clinical trials in regards to the problems that volunteers face. Additionally, even if those who design the trials know the problem, it is still difficult to apply that understanding to designing a better clinical trial. To fix this problem, this project is meant to use a VR experience that simulates the patient experience and allows for the individuals who design clinical trials to better understand and fix the issues that many volunteers face. The VR experience is meant to allow for healthcare providers to empathize with the volunteers and find solutions from a more personal perspective.

The first part of the project will focus on more pure consulting work where we provide recommendations on what sources to collect the VR data from and an overview of the VR space in healthcare as the Roche team separately builds out the MVP for the Immersive VR Patient Product. The second part of the project will focus on helping to code the VR experience and analyze the data collected from the MVP.",The Immersive Patient Experience Project is meant to improve the quality of life for volunteers who participate in clinical trials.,Python,Python,**KPIs**: Not explicitly mentioned in the document.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S23_Compassion_Case.pdf,S23,**Case Name**: Compassion A,Compassion International,"This case is a continuation of work done last semester by an HDAG case team. Compassion supports specific partner churches or communities through Complementary Interventions (CIVs). While CIVs are well-documented, there is a gap in the categorization available for analytics. Building upon the work from last semester, HDAG will develop a model to measure how CIV sub-categories determined by the Fall 2022 team align with a set of 22 outcome measures provided by Compassion. Then, HDAG will use this model to identify geographic need clusters where children are widely underserved.","This case is a continuation of work done last semester by an HDAG case team. Compassion supports specific partner churches or communities through Complementary Interventions (CIVs). While CIVs are well-documented, there is a gap in the categorization available for analytics. Building upon the work from last semester, HDAG will develop a model to measure how CIV sub-categories determined by the Fall 2022 team align with a set of 22 outcome measures provided by Compassion. Then, HDAG will use this model to identify geographic need clusters where children are widely underserved.","Compassion International is an American child sponsorship and humanitarian aid organization headquartered in Colorado Springs, Colorado, that aims to positively influence the long-term development of children globally who live in poverty.","Python, R","**Tech Stack**: Python, R (optional)",**KPIs**: 22 outcome measures provided by Compassion; correlation analysis framework for completed CIVs with outcome survey results; visual representation of geographic need clusters.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S23_Roche_Case.pdf,S23,Roche A,Roche,Develop machine learning models to predict future service activities for any given customer/instrument/geography during a specific time interval.,"Roche engineers regularly onsite complete customer “service activities” onsite to perform both planned and unplanned maintenance on medical instruments. The objective of this case is to develop machine learning models to predict future service activities for any given customer/instrument/geography during a specific time interval (e.g. month, week, day). This will afford Roche the opportunity to better plan how/when service could be performed and better accommodate future staffing levels to meet the demands of future service activities.

Exploratory data analysis (EDA) will be conducted to identify and quantify the likelihood of predictive characteristics, attributes, and model focus areas. Unsupervised clustering will also be performed to identify additional patterns. Time series modeling will be leveraged to predict the likelihood of future service activities. The predicted likelihood of future service activity over time will be reported by customer and by instrument, and the predicted volume of future service activity will be reported based on geographic region.","The outcome of the project is to develop machine learning models to predict future service activities for Roche's medical instruments, allowing better planning and staffing to meet service demands.",Python,**Tech Stack**: Python,"**KPIs**: The predicted likelihood of future service activity over time will be reported by customer and by instrument, and the predicted volume of future service activity will be reported based on geographic region."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S23_CincyTech_Case.pdf,S23,CincyTech,CincyTech,"The project goal is to analyze the VC round, deal type, deal amount, total invested capital, pre-money valuation, and post valuation for each company in multiple sectors. The data will be compiled over the last 10 years by quarters, grouping the companies by industry and analyzing the general trends and casual relationships between the features provided by Pitchbook. A model will be developed to find the determinants of a “successful” exit backed by a VC firm, focusing on the variance between pre-money and post valuation, and understand the changes throughout history. Exploratory data analysis will be conducted to quantify the information based on each category bucket.","The project goal is to analyze the VC round, deal type, deal amount, total invested capital, pre-money valuation, and post valuation for each company in multiple sectors. The data will be compiled over the last 10 years by quarters, grouping the companies by industry and analyzing the general trends and casual relationships between the features provided by Pitchbook.

A model will be developed to find the determinants of a “successful” exit backed by a VC firm, focusing on the variance between pre-money and post valuation, and understand the changes throughout history. Exploratory data analysis will be conducted to quantify the information based on each category bucket.","The project goal is to analyze the VC round, deal type, deal amount, total invested capital, pre-money valuation, and post valuation for each company in multiple sectors.",Python,Python,"KPIs: VC round, deal type, deal amount, total invested capital, pre-money valuation, post valuation, determinants of a “successful” exit, variance between pre-money and post valuation, trends and causal relationships between features."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S23_CVC_Case.pdf,S23,Cambridge Volunteer Clearinghouse (CVC),Cambridge Volunteer Clearinghouse (CVC),"The goal of this project will be to measure and quantify the decline in volunteerism in the Cambridge community, and more broadly in other communities. The work will help nonprofits take informed action to counter the trend by adjusting program structure or offering volunteer roles to accommodate permanent changes in volunteer expectations and needs.","The goal of this project will be to measure and quantify the decline in volunteerism in the Cambridge community, and more broadly in other communities. The work will help nonprofits take informed action to counter the trend by adjusting program structure or offering volunteer roles to accommodate permanent changes in volunteer expectations and needs. The HDAG team will begin by designing surveys to investigate volunteerism decline in the local Cambridge community, leveraging statistical inference techniques for proper survey design and implementing qualitative analysis on responses. The websites of other volunteer agencies will be scraped to capture a more granular view of the volunteerism landscape in the Boston community.

The team will then consider more broadly the trends in volunteerism across the US by analyzing large-scale data from AmeriCorps, an independent government agency supporting national service and volunteerism. Longitudinal modeling and trend analysis will be techniques particularly relevant to this endeavor. The team will in particular seek to characterize and quantify the overall reduction in volunteerism due to the pandemic.","The goal of this project will be to measure and quantify the decline in volunteerism in the Cambridge community, and more broadly in other communities. The work will help nonprofits take informed action to counter the trend by adjusting program structure or offering volunteer roles to accommodate permanent changes in volunteer expectations and needs. The HDAG team will begin by designing surveys to investigate volunteerism decline in the local Cambridge community, leveraging statistical inference techniques for proper survey design and implementing qualitative analysis on responses. The websites of other volunteer agencies will be scraped to capture a more granular view of the volunteerism landscape in the Boston community. The team will then consider more broadly the trends in volunteerism across the US by analyzing large-scale data from AmeriCorps, an independent government agency supporting national service and volunteerism. Longitudinal modeling and trend analysis will be techniques particularly relevant to this endeavor. The team will in particular seek to characterize and quantify the overall reduction in volunteerism due to the pandemic.",Python,**Tech Stack**: Python,**KPIs**: Measure and quantify the decline in volunteerism in the Cambridge community and other communities; adjust program structure or volunteer roles based on findings; analyze trends in volunteerism across the US; characterize and quantify the overall reduction in volunteerism due to the pandemic.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S24_CAC_Case.pdf,S24,Coaching Association of Canada,Coaching Association of Canada,"The main goals are: 1) to support the CAC in a holistic evaluation of its current E-Learning modules available for its coaches and 2) to help them analyze and find key insights from their to-be completed coaching demographics survey. More specifically, the project will: Conduct a thorough demographic analysis of current E-Learning users based on existing datasets to provide suggestions for pricing, user retention, and target audience; Analyze and suggest improvements to the User Experience (UX) and accessibility of the E-Learning platform; (2nd half of the semester) Analyze the completed coaching demographics survey to generate visualizations, insights, and potential recommendations for the CAC.","The main goals are: 1) to support the CAC in a holistic evaluation of its current E-Learning modules available for its coaches and 2) to help them analyze and find key insights from their to-be completed coaching demographics survey. More specifically, the project will:

●     Conduct a thorough demographic analysis of current E-Learning users based on existing datasets to
provide suggestions for pricing, user retention, and target audience
●     Analyze and suggest improvements to the User Experience (UX) and accessibility of the E-Learning
platform
●     (2nd half of the semester) Analyze the completed coaching demographics survey to generate
visualizations, insights, and potential recommendations for the CAC","The main goals are: 1) to support the CAC in a holistic evaluation of its current E-Learning modules available for its coaches and 2) to help them analyze and find key insights from their to-be completed coaching demographics survey. More specifically, the project will:

●     Conduct a thorough demographic analysis of current E-Learning users based on existing datasets to
provide suggestions for pricing, user retention, and target audience
●     Analyze and suggest improvements to the User Experience (UX) and accessibility of the E-Learning
platform
●     (2nd half of the semester) Analyze the completed coaching demographics survey to generate
visualizations, insights, and potential recommendations for the CAC",Python,Python,"**KPIs**: Support the CAC in a holistic evaluation of E-Learning modules, conduct demographic analysis of E-Learning users, analyze and suggest improvements to User Experience (UX) and accessibility, generate visualizations and insights from coaching demographics survey."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S24_CocaCola_Case.pdf,S24,Coca-Cola,Coca-Cola,"The objective of this case is to conduct a thorough analysis of the market fit for Coca-Cola by leveraging both internal company data and external market information. This analysis will encompass various aspects including brand performance, distribution channels, engagement models, competitive landscape, and financial performance. The key focus is to identify strengths and weaknesses in their Asia-Pacific region and to provide valuable insights for informed decision-making.","The objective of this case is to conduct a thorough analysis of the market fit for Coca-Cola by leveraging both internal company data and external market information. This analysis will encompass various aspects including brand performance, distribution channels, engagement models, competitive landscape, and financial performance. The key focus is to identify strengths and weaknesses in their Asia-Pacific region and to provide valuable insights for informed decision-making.","The outcome of the project is to utilize predictive analysis techniques, including machine learning, to create an interactive dashboard that provides data visualizations displaying key performance metrics with user-tweaked input values to simulate possible future outcomes.","Python/Excel, Tableau","Python/Excel, Tableau (optional)","- Industry market research and analyze brand performance to assess market recognition
- Assess the revenue generated to identify areas of improvement and optimization"
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S24_Eckerd_Case.pdf,S24,Eckerd Connects,Eckerd Connects,"Using internal data sources from Eckerd Connects, HDAG would help to determine factors contributing to effective recruitment and retention for Job Corps, a program that provides education and career training to low-income individuals. Specific project tasks may include exploratory analysis to determine students’ prospects when entering the program, factors contributing to success, reasons for leaving, etc., and a dashboard for monitoring students during the program and shortly after leaving (90 days).","Using internal data sources from Eckerd Connects, HDAG would help to determine factors contributing to effective recruitment and retention for Job Corps, a program that provides education and career training to low-income individuals. Specific project tasks may include exploratory analysis to determine students’ prospects when entering the program, factors contributing to success, reasons for leaving, etc., and a dashboard for monitoring students during the program and shortly after leaving (90 days).","Outcome: Eckerd Connects is a non-profit organization committed to providing solutions for struggling families and sharing these solutions. They focus on effective recruitment and retention for Job Corps, analyzing factors contributing to success and reasons for leaving, and developing a monitoring dashboard for students.",Python,Python,"**KPIs**: Factors contributing to effective recruitment and retention for Job Corps, students’ prospects when entering the program, factors contributing to success, reasons for leaving, monitoring students during the program and shortly after leaving (90 days)."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S23_TrueValue_Case.pdf,S23,True Value,True Value,"True Value currently sets wholesale prices (price to their customers) using a rule-based framework that positions price within “acceptable bands” from its primary competitors. The project goal is to model demand for True Value’s products over 2021 and 2022 and estimate demand elasticities with respect to certain variables within the model. A rough outline of the project may look like: Exploratory Data Analysis: Capture correlative factors for True Value’s demand, including, but not limited to, comprehensive order history data, competitor pricing, and industry trends; Demand Modeling: Developing a model to predict the responsiveness of demand to pricing variables based on historic order activity and other related datasets; Recommendations for future alterations to their pricing model based on simulated scenarios outlining expected demand response to proposed True Value initiatives to maximize profitability and growth.","True Value currently sets wholesale prices (price to their customers) using a rule-based framework that positions price within “acceptable bands” from its primary competitors. The project goal is to model demand for True Value’s products over 2021 and 2022 and estimate demand elasticities with respect to certain variables within the model. A rough outline of the project may look like: ● Exploratory Data Analysis: Capture correlative factors for True Value’s demand, including, but not limited to, comprehensive order history data, competitor pricing, and industry trends ● Demand Modeling: Developing a model to predict the responsiveness of demand to pricing variables based on historic order activity and other related datasets ● Recommendations for future alterations to their pricing model based on simulated scenarios outlining expected demand response to proposed True Value initiatives to maximize profitability and growth. Internal Partners: ● Ron Byczynski (Division Vice President, Infrastructure, DevOps and QA) Datasets: 1. Order History: data pertaining to True Value sales, before and after changing and rolling out initiatives that affect different variables in their demand model 2. Competitor Data: various referential attributes associated with competitors in the industry and their product/sales information 3. Industry Trends: HDAG will supplement any data provided by True Value with publicly available data on the economy and hardware industry that is expected to influence True Value’s demand Coding Languages: Python Specific Skills 1. Data Analysis: Analyzing the provided dataset to come up with useful interpretations and outtakes. Communicating the limits and findings of any model(s) to the client 3. NLP and Model Creation: Creating a group of models using NLP and other Machine Learning methods to derive insights from the datasets. Expected Technical Difficulty: Intermediate/Advanced",The project goal is to model demand for True Value’s products over 2021 and 2022 and estimate demand elasticities with respect to certain variables within the model.,Python,Python,"**KPIs**: Demand elasticities, pricing responsiveness, profitability, growth"
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S24_CTT_Case.pdf,S24,"CTT Correios de Portugal, S.A.","CTT Correios de Portugal, S.A.","CTT seeks a collaboration with HDAG to optimize its operational capabilities. Through a series of consultations, CTT has expressed an interest in utilizing predictive modeling to optimize and understand its operational/delivery capabilities. The broader objective is to integrate dynamic, data-driven strategies to improve service efficiency, ensure timely deliveries, and subsequently enhance customer satisfaction.","CTT seeks a collaboration with HDAG to optimize its operational capabilities. Through a series of consultations, CTT has expressed an interest in utilizing predictive modeling to optimize and understand its operational/delivery capabilities. The broader objective is to integrate dynamic, data-driven strategies to improve service efficiency, ensure timely deliveries, and subsequently enhance customer satisfaction. HDAG will work on:

1. Predictive modeling with feature importance
2. Capacity visualization
3. Creating an operational dashboard

Internal Partners: Chief Data Officer

Datasets:

1. Historical data on delivery capacity at various stations across Portugal and Spain
2. Information on the number of vehicles, personnel, volume of parcels handled, space, weight, etc., at each delivery point.
3. Any additional data on the resources required to handle the parcels.

Coding Languages: Python

Specific Skills
1. Modeling and prediction
2. Data visualization
3. Dashboarding

Expected Technical Difficulty: Medium","CTT seeks a collaboration with HDAG to optimize its operational capabilities. Through a series of consultations, CTT has expressed an interest in utilizing predictive modeling to optimize and understand its operational/delivery capabilities. The broader objective is to integrate dynamic, data-driven strategies to improve service efficiency, ensure timely deliveries, and subsequently enhance customer satisfaction. HDAG will work on:

1. Predictive modeling with feature importance
2. Capacity visualization
3. Creating an operational dashboard",Python,Python,**KPIs**: Revenue of EUR 715.42 million for the first nine months of 2023.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S24_Guideline_Case.pdf,S24,Case Name: Guideline Central Sourcing Project,Guideline Central,"Guideline Central maintains the largest healthcare guideline library in the world; they work with medical societies across the globe to offer guidelines online, in their mobile app, and via their APIs AI-driven search to help the public find the right medical information at the right time.","Guideline Central maintains the largest healthcare guideline library in the world; they work with medical societies across the globe to offer guidelines online, in their mobile app, and via their APIs AI-driven search to help the public find the right medical information at the right time. This project has two components: 1. Guidelines Review and Public Comment System 2. Guidelines extraction chatbot. Internal Partners: Vice President. Datasets: Internal Guideline Central data. Coding Languages: Python, Microsoft Word macros. Specific Skills: Document analysis, Natural language processing, Large language models, retrieval augmented generation. Expected Technical Difficulty: Medium/Advanced.","Guideline Central maintains the largest healthcare guideline library in the world; they work with medical societies across the globe to offer guidelines online, in their mobile app, and via their APIs AI-driven search to help the public find the right medical information at the right time.","Python, Microsoft Word macros","Python, Microsoft Word macros",**KPIs**: Not provided in the document.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S24_MLT_Case.pdf,S24,Management Leadership for Tomorrow,Management Leadership for Tomorrow,"This project would involve using public data sources to determine the key drivers of racial wealth gaps and investigate interventions to close them. The first stage of the project would involve exploratory data analysis and data processing while in the second stage of the project, an interactive dashboard simulating how changes in variables impact wealth gaps would be created. The findings would help MLT pitch to philanthropic donors by providing them with a better understanding of the significance of MLT’s programming.","This project would involve using public data sources to determine the key drivers of racial wealth gaps and investigate interventions to close them. The first stage of the project would involve exploratory data analysis and data processing while in the second stage of the project, an interactive dashboard simulating how changes in variables impact wealth gaps would be created. The findings would help MLT pitch to philanthropic donors by providing them with a better understanding of the significance of MLT’s programming.",Management Leadership for Tomorrow (MLT) is a non-profit organization that is dedicated to closing the racial wealth gap in the United States. They provide career development programs to underrepresented minorities at both the early and mid-career stages and collaborate with corporate partners to build diverse leadership pipelines.,Python or R,Python or R,**KPIs**: Not specified in the document.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S24_QuartzBio_Case.pdf,S24,Case Name: QuartzBio Sourcing Project,QuartzBio,"This project has three components:

1. Ad-hoc interrogation across systems (e.g., Excel, Smartsheet, Salesforce) – questions like the following:

●     What products are covered under X client and what is the revenue by product YTD?
●     What clients have the highest and lowest revenue-to-labor ratios?
●     Which clients have the highest rates of missed late / deliverables? (or early / on-time deliverables)
○    Open-source LLMs and retrieval-augmented generation (RAG) pipelines would need to be
created and integrated with these systems

2. Monthly Revenue Import Process

●     The current process is to have someone open all the individual revenue files and enter the monthly
units for each revenue line in the system one by one. That individual will also confirm that the ERP
system, individual revenue file, and the portfolio backlog workbook are in alignment with monthly
revenue during this process. This is a timely process and exposes Quartz to human error.
●     Build a method for Quartz’s internal teams to review the revenue and approve the units before the
upload takes place. Interested in seeing how they could use this upload process to streamline their
month-end close process without losing track of revenue and forecast on each project.

3. Automation of business and project delivery metrics, dashboards, and reporting

●     Create automated dashboards using existing, defined metrics for business reporting packages
(currently fully manual)
●     Standard Reporting – currently in Excel and PowerPoint; ~20 reports/month from 5-6 data sources
●     SaaS Reporting – currently in Excel and PowerPoint; ~15 reports/month from 5-6 data sources

Internal Partners: Executive Director, Operations

Datasets: Internal QuartzBio data

Coding Languages: Python, Excel, etc.

Specific Skills
1.    Large language models and retrieval augmented generation
2.    Data visualization and dashboarding

Expected Technical Difficulty: Advanced","This project has three components:

1. Ad-hoc interrogation across systems (e.g., Excel, Smartsheet, Salesforce) – questions like the following:

●     What products are covered under X client and what is the revenue by product YTD?
●     What clients have the highest and lowest revenue-to-labor ratios?
●     Which clients have the highest rates of missed late / deliverables? (or early / on-time deliverables)
○    Open-source LLMs and retrieval-augmented generation (RAG) pipelines would need to be
created and integrated with these systems

2. Monthly Revenue Import Process

●     The current process is to have someone open all the individual revenue files and enter the monthly
units for each revenue line in the system one by one. That individual will also confirm that the ERP
system, individual revenue file, and the portfolio backlog workbook are in alignment with monthly
revenue during this process. This is a timely process and exposes Quartz to human error.
●     Build a method for Quartz’s internal teams to review the revenue and approve the units before the
upload takes place. Interested in seeing how they could use this upload process to streamline their
month-end close process without losing track of revenue and forecast on each project.

3. Automation of business and project delivery metrics, dashboards, and reporting

●     Create automated dashboards using existing, defined metrics for business reporting packages
(currently fully manual)
●     Standard Reporting – currently in Excel and PowerPoint; ~20 reports/month from 5-6 data sources
●     SaaS Reporting – currently in Excel and PowerPoint; ~15 reports/month from 5-6 data sources

Internal Partners: Executive Director, Operations

Datasets: Internal QuartzBio data

Coding Languages: Python, Excel, etc.

Specific Skills
1.    Large language models and retrieval augmented generation
2.    Data visualization and dashboarding

Expected Technical Difficulty: Advanced","Outcome: The project aims to enhance data interrogation across systems, streamline the monthly revenue import process, and automate business and project delivery metrics, dashboards, and reporting for QuartzBio.","Python, Excel, etc.","Python, Excel, etc.","**KPIs**: Revenue by product YTD, revenue-to-labor ratios, rates of missed late/deliverables, alignment of ERP system and revenue files, approval of revenue units, automation of business and project delivery metrics, number of reports generated per month."
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S24_SharkNinja_Case.pdf,S24,**Case Name**: SharkNinja Case,SharkNinja,"SharkNinja specializes in small household appliances in 24 different countries, with two major brands: Shark (vacuums, hair dryers, fans) and Ninja (grills, coffee makers, blenders). The project involves the Strategic Sales and Analytics Team, focusing on the impact of major deals on trade, retailer pricing behavior, and consumption modeling for the Ninja Thirsti product.","This SharkNinja Case is involved with the Strategic Sales and Analytics Team (note: weekly meetings/deliverables with the client). A few high-level goals outlined (but not limited to) are: 1. What is the impact of Club and QVC/HSN major deals on the rest of trade? SharkNinja moves a lot of volume in a short period of time via clubs (ie Costco) or home shopping (QVC or HSN). This must have an impact on the rest of the trade (WMT, Target, etc). What is it? How long does it last? Are we simply just pulling sales forward at a lower margin? Create modeling to help SharkNinja find the end “answer” or “answers,” allowing them to reuse the model later. 2. Are retailers gaming our UPP (unilateral price policy)? SharkNinja sets a price floor on its products, and via their UPP, if a retailer chooses to price the product below the given floor, we stop shipping them. Depending on how much inventory a retailer is carrying, this stopped shipment can be meaningful (the retailer will lose sales) or it can be inconsequential (they have enough inventory to last them through the stop shipment). Are there clear patterns by each retailer on how they are ordering and pricing, allowing them to “game” this? Propose possible changes or order fulfillment or their UPP enforcement to discourage behavior. 3. How can we get smarter with the consumption modeling of our Ninja Thirsti? Our Ninja Thirsti utilizes CO2 cylinders and Flavor pods, requiring a consumer to replenish both. Possible areas to explore: i. How do we forecast this demand? ii. Are consumers “dropping out” ie they bought the product, used it some, but now aren’t replenishing it? iii. How frequently are consumers replenishing? iv. Regional differences?","Outcome: The project aims to analyze the impact of major deals on trade, investigate retailer behavior regarding pricing policies, and improve consumption modeling for the Ninja Thirsti product.",**Programming Languages Used**: Not specified in the document.,**Tech Stack**: Not provided in the document.,**KPIs**: 1. Impact of Club and QVC/HSN major deals on the rest of trade. 2. Retailers gaming UPP (unilateral price policy). 3. Consumption modeling of Ninja Thirsti.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/cases/S24_VLS_Case.pdf,S24,VLS Environmental Solutions,"VLS Environmental Solutions, LLC","The primary goal of this project is to identify and provide leads for the sales force of the manufacturing waste management team of VLS. The final findings of the project would reside in their Customer Relationship Management (CRM) system. Specific tasks include exploratory data analysis of internal data sources and public data sources to be identified by the team, determining the common factors of clients that are typically willing to pay more for environmental services like the ones VLS provides, and investigating the potential of expanding into regions VLS is not yet operating in.","The primary goal of this project is to identify and provide leads for the sales force of the manufacturing waste management team of VLS. The final findings of the project would reside in their Customer Relationship Management (CRM) system. Specific tasks include: exploratory data analysis of internal data sources and public data sources to be identified by the team, determining the common factors of clients that are typically willing to pay more for environmental services like the ones VLS provides, and investigating the potential of expanding into regions VLS is not yet operating in.",The primary goal of this project is to identify and provide leads for the sales force of the manufacturing waste management team of VLS.,Python,Python,**KPIs**: The final findings of the project would reside in their Customer Relationship Management (CRM) system.
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/F20_Schneider_Final.pptx,F20,,"Slide #0: 

 Image: a man and a woman using a cell phone

Confidential Property of Schneider Electric  
A Collaboration Between Schneider Electric Indirect Procurement  & Harvard Analytics

Dec. 9th, 2020
Project Lasik
Executive Review


Slide #1: 
Table of Contents
Page ‹#›
Confidential Property of Schneider Electric |
Project Background & Overview 
Deliverable Update
Next Steps
1
2
3
Appendix: More About Project Lasik
4


Slide #2: 
Page ‹#›
Confidential Property of Schneider Electric |
Problem Statement(s):

1. Indirect Procurement (IP) spend analytics thus far has been back-forward looking, as descriptive analytics; IP teams struggled when pressed for spend projection taking consideration macro/microeconomics associated with the industry, competitive landscape, and Schneider Electric current and projected financial results.

2. IP spends significant time “cleansing” spend data before data are “validated” for use; in many cases, spend data are “mis-categorized” from the start (Purchase Requisition, or PR creation by Schneider users). No tangible mechanism is place today to improve spend data integrity at point of entry (PR creation).

Desired Outcome for 2 months of Harvard Analytics collaboration:

1. A “predictive analytics” design foundation that can be implemented to enable “predictive” spend analytics going forward
2. A design foundation that can be implemented in scalable manner to enhance data integrity at point of entry/creation (PR)
Project Background & Overview 
Gather insights into Schneider IP spend, to enable predictive analytics for future spend, and introduce potential data integrity improvement design to the purchase request (PR) process.


Slide #3: 
Page ‹#›
Confidential Property of Schneider Electric |
Project Execution Approach
“Agile” scope definition and bi-weekly check-ins/milestone reviews enables close alignment in exploration and design execution  
1. Define Preliminary Scope for Collaboration
2. Align on Pre-dictive IP Spend Deliverable
3. Align on IP Spend Data Integrity Improvement Deliverable
  4. Review 
Deliverables
5. Close Out Project; Transition Knowledge
Bi-weekly Check-ins/Milestone Reviews
Part 1 – Predictive Analytics to Forecast Schneider IP Spend

Leverage public financial data (Schneider + Schneider Peer Group + Industry Verticals)
Utilize outside company markers to reveal trends and predictions for Schneider IP spend
Linear regression to identify correlation between various “financial” markers and IP spend based on historical spend data, based on which IP spend predictions can be continuously tweaked and improved

Part 2 - Purchase Requisition (PR) Data Integrity Improvement at Entry Point
Simplify manual sanity checks for verifying purchase requisitions; establish foundation to enable automated validation process via bots going forward
Highlight errors in typical material group classification by requestors – identified as area for future RPA + manual combination solution
Identify key words & frequency to enable near-term gain of e-Catalog establishment effort


Slide #4: 
Page ‹#›
The Outcome
Part 1 – Predictive Analytics to Forecast Schneider IP Spend


 Image: a collage of images of a computer screen

Linear Regression to Identify IP Spend Projection Algorithm

 Image: a collage of photos showing different types of food


 Image: a collage of photos showing different types of computers



Slide #5: 
Page ‹#›
The Next Steps
Part 1 – Predictive Analytics to Forecast Schneider IP Spend

Currently IP spend data from 2018 – 2020 have been incorporated; team will incorporate 2015-2017 data after “cleansing”
Further define relevant financial data “marker” to improve validity of projection algorithm
Refine peer group (Legrand, Siemens, Rockwell Automation, Vertiv) to improve validity of project algorithm
Transition project learning and current model to GSC Analytics to carry the effort forward


Slide #6: 
Page ‹#›
The Outcome

Part 2 - Purchase Requisition (PR) Data Integrity Improvement at Entry Point


 Image: a black and white photo of a computer screen


 Image: a computer screen with a picture of a person on it


 Image: a black and white photo of a computer screen


 Image: a black and white photo of a tv screen





Slide #7: 
Page ‹#›
The Next Steps
Part 2 - Purchase Requisition (PR) Data Integrity Improvement at Entry Point

Refine the “truth” “flagged” and “dirty” findings and rules; build bot(s) to “crawl” across PRs created in Coupa, to self-correct or flag for “human intervention”, so spend data integrity can be “controlled” or at least “enhanced” at point of entry
Transition project learning and current model to GSC Analytics to carry the effort forwar
Build key words and frequency findings into e-Catalog deployment ongoing to achieve quick win



Slide #8: 

Part 1: Methodology, Outcome, and Benefits 
Part 2: Methodology, Outcome, and Benefits
Moving Forward
Page ‹#›
Appendix
More About Project Lasik 


Slide #9: 
Page ‹#›
Confidential Property of Schneider Electric |
Gathered public semesterly IP spend data from 2015-2020
Company markers (datasets are projections, e.g. projected revenue for 2022): 
Market Cap 
Free Cash Flow 
Diluted EPS 
Revenue 
Trained a linear regression model to predict IP spend in 2021-2022
Required complete datasets for other variables to 2022 to predict IP spend
Filled cells are inputted into the model (ignore time data)
Green indicates historical values
Yellow indicates forecasts




Part 1 Methodology

Aimed at forecasting spend amounts based on various variables.


Slide #10: 
Page ‹#›
Confidential Property of Schneider Electric |
Part 1 Methodology



Slide #11: 
Page ‹#›
Confidential Property of Schneider Electric |
Part 1 Outcomes

Linear Regression model to return IP spend forecasts for the years 2021-2022 

 Image: a collage of photos showing different types of electronic devices


 Image: a collage of photos showing different types of electronic devices


Output: IP Spend predictions for semester periods 12/20, 6/21, 12/21, 6/22, 12/22 

Input: Public financial data forecasts for future semester periods from financial institutions & (not pictured) historical IP spend and public financial data


Slide #12: 
Page ‹#›
Confidential Property of Schneider Electric |
Forecasted data can be used to influence company decisions to reduce future IP spend 
Could add more financial data as necessary to improve accuracy 
Model is publicly available on Microsoft Azure 
Part 1 Benefits

Easily accessible and customizable model to create IP spend predictions


Slide #13: 
Page ‹#›
Confidential Property of Schneider Electric |
Draw associations of vendors, keywords (item description), and material group in the validated purchase requests
Apply determined associations in un-validated purchase requests
Utilize keyword frequencies as well as each supplier’s set of material groups to highlight potentially wrong material group classifications by requestor
Part 2 Methodology

Aimed at flagging errors in the purchase requests using verified purchase requests.


Slide #14: 
Page ‹#›
Confidential Property of Schneider Electric |
Un-validated purchase requests are indicated with “unknown vendor” or “flagged”
Marks potential errors in the un-validated purchase requests
Threshold of 10 times for frequency of keywords
Script allows specification of “validated” and “un-validated” purchase requests csv files
Output of “un-validated” purchase requests can simplify validation process

Part 2 Outcomes

Script to filter out unknown vendors and flag potentially wrong material group classifications.


Slide #15: 
Page ‹#›
Confidential Property of Schneider Electric |
Lightweight Python script to append an indicator column to the un-validated purchase requests
Easy to update with new set of validated purchase requestors
Output of “common_keywords” which shows high frequency keywords for all the material codes & “all_keywords” which shows all the keywords for each material code

Part 2 Benefits

Streamline purchase request validation process.


Slide #16: 
Page ‹#›
Confidential Property of Schneider Electric |
Moving Forward 
Part 1: 
Gather more fine-tuned data, either quarterly or monthly 
Improve accuracy of prediction with dataset increase
Incorporate time series data to enhance prediction
Spend data should be updated to reflect accurate amounts in earlier years 
Part 2: 
Look for more trained and testing data with more diverse vendors 
Current model flags a large number of vendors as unknown
Build upon the logic for improved flagging for relative frequency
Improve on the model so it returns lower percentage of false negatives
Change the model to include relative frequency 
Potential improvements.


Slide #17: 
Page ‹#›
Confidential Property of Schneider Electric |
Moving Forward Continued
Part 1: 
Access Microsoft Azure environment via https://gallery.cortanaintelligence.com/Experiment/Part-1-Linear-Regression-on-IP-Spending
Input csv of previous IP spend and financial data as well as financial forecasts to run model
Elements to personalize:
Load data: historical IP spend and financial data & financial forecasts
Split data: decimal percentage of the historical data
Part 2: 
Specify xls files for the un-validated and validated purchase requests
Run “parse_spend.py” then “flag_unknowns.py” to received “flagged.csv” reports
Elements to personalize:
Set of un-validated purchase requests to check
Set of validated purchase requests to establish the “truth” 
Guidelines of hand-off.",,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/F20_[SportsAnalytics]_Final.pptx,F20,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/F22_Moderna_Final.pptx,F22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/F22_SouthPole_Final.pptx,F22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/F23_CAC_Final.pptx,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/F23_LDF_Final.pptx,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/F23_ScanGlobal_Final.pptx,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/F23_USAG_Final.pptx,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/F23_WooSox_Final.pdf,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/S21_ThinkCERCA_Final.pptx,S21,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/S21_UnderArmour_Final.pptx,S21,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/S22_ACE_Final.pdf,S22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/S22_ACE_Final.pptx,S22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/S22_Activision_Final.pptx,S22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/S22_Greystar_Final.pdf,S22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/S22_Roche_Final.pptx,S22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/S22_Siemens_Midpoint.pptx,S22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/S23_Compassion_Final.pptx,S23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F22_Compassion_Proposal.pdf,F22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F22_FemTech_Proposal.pdf,F22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F22_Invitae_Proposal.pdf,F22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F22_Moderna_Proposal.pdf,F22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F22_Roche_Proposal.pdf,F22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F22_SouthPole_Proposal.pdf,F22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F22_ThermoFisher_Proposal.pdf,F22,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F23_CocaCola_Proposal.pdf,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F23_Merck_Proposal.pdf,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F23_Moderna_Proposal.pdf,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F23_NAACP_Proposal.pdf,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F23_Octane_Proposal.pdf,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F23_Philadelphia_Proposal.pdf,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F23_ScanGlobal_Proposal.pdf,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F23_SkyGrid_Proposal.pdf,F23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/F24_FloridaGators_Proposal.pdf,F24,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/S23_Bluebonnet_Proposal.pdf,S23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/S23_CincyTech_Proposal.pdf,S23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/S23_Compassion_Proposal.pdf,S23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/S23_CVC_Proposal.pdf,S23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/S23_Galaxy_Proposal.pdf,S23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/S23_Roche_Proposal.pdf,S23,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/S24_CCT_Proposal.pdf,S24,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/proposals/S24_SharkNinja_Proposal.pdf,S24,,,,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/F20_UNIDO_Final.pptx,F20,,"Slide #0: 
Programs for Impact: Quantifying UNIDO’s Contributions to the Sustainable Development Goals 
How the IRPF links UNIDO programs to SDG results achievement
January 2021
HDAG Group 2021 ® All Rights Reserved.

 Image: a black and white photo of a sign on a white wall


 Image: a large building with several flags on top of it



Slide #1: 
Glossary of Acronyms

UNIDO
United Nations Industrial Development Organization
IRPF
Integrated Results and Performance Framework
ISID
Inclusive and Sustainable Industrial Development
SDGs
Sustainable Development Goals


00


Slide #2: 
Executive Summary
3
Overview
01


Slide #3: 
1: What is the IRPF? 
5
Challenge:
The United Nations uses results-based management (RBM) systems to improve programs and activities and increase effectiveness and accountability for results and performance
In 2019, UNIDO adopted a new Integrated Results Performance Framework (IRPF), an RBM system
Complements Medium Term Program Framework (MTPF) 2018-2021
Based on a shift from project-based logical framework approach to a Theory of Change (ToC) model
IRPF lays out projected achievements in the short, medium, and long term, to be monitored and reported via performance indicators at several levels (continuously fine-tuned)
IRPF indicators provide a qualitative and quantitative measure of UNIDO’s contributions to ISID and SDGs

 Image: an advertisement for a new york magazine



Slide #4: 
1: What are the Sustainable Development Goals?
5
“The Sustainable Development Goals (SDGs) constitute the core of the 2030 Agenda for Sustainable Development and guide all global, regional and national development endeavours for the next 15 years. The United Nations Industrial Development Organization (UNIDO) is fully committed to contributing to the achievement of the SDGs, while delivering on its mandate to support Member States in achieving inclusive and sustainable industrial development (ISID)...Due to the interlinked nature of the SDGs, many of UNIDO’s activities contribute to more than one SDG.” – UNIDO
Challenge:

 Image: a series of photos showing different types of street signs



Slide #5: 
1: How can UNIDO’s contributions to the SDGs be quantified?
5

Challenge:

 Image: an advertisement for a new york magazine


UNIDO IRPF Guide, 2020


Slide #6: 
Executive Summary
3
Overview
02


Slide #7: 
2: Through a review of evidence, links between the ISID and SDGs can be characterized
12
Data Insights: ODI literature-based matrix
Categorized 9 IRPF areas and 17 SDGs, reviewed over 250 studies for evidence, and divided the evidence into 29 cases exploring ISID-SDG links
For each connection, identified direction (type of impact, positive/negative) and strength (0 to 3 based on quality and quantity of evidence) of link
Overall results point to strong and positive connection between ISID and SDGs
Relationship often strongly positive, but in some cases weakly significant
Some areas where contribution of ISID to SDGs still to be proven

 Image: a collage of photographs of a person on a computer

Excerpt from matrix of coefficients 


Slide #8: 
2: Connecting existing SDG Indicators to IRPF Indicators
12
Data Insights: Global SDG Indicators Database
Mapped indicators from Levels 1 (UNIDO-related SDG indicators) and 2 (ISID and related SDG indicators) of the IRPF to each results area according to “Managing for Results: A Guide to UNIDO’s Integrated Results and Performance Framework Approaches and Tools”
EX: 1.3 SDG 9.2.2: Manufacturing employment as a proportion of total employment mapped to Decent Jobs results area
Equivalents for new IRPF indicators without documented data were found among SDG indicators, e.g. ENV.3: Cumulative improved energy efficiency and SDG 7.3.1: By 2030, double the global rate of improvement in energy efficiency
Data on these indicators found in the UN Global SDG Indicators Database

 Image: a collage of photos of various types of writing



Slide #9: 
Executive Summary
3
Overview
03


Slide #10: 
A single representative “change score” was calculated for each ISID area. Then, using the matrix of coefficients (literature-based), we can predict the impact of ISID results area changes on SDGs.
12
3: Predicted SDG impacts by ISID results area

 Image: a series of photographs showing different types of objects

“Reduced emissions” and “Decent Jobs”  areas are predicted to negatively impact SDGs
“Productivity” predicted to have greatest positive impact.


Slide #11: 
12
Source: HDAG analysis of SDG indicators data
We can find the correlations between our indicator datasets and the SDGs from year to year...
3: SDG vs ISID correlations and regressions

 Image: a blue and white sheet of paper with numbers on it

As well as the implied regression coefficients between the two, an implicit comparison of how much a one standard deviation change in each indicator affects each SDG

 Image: a series of photographs showing different types of writing

Results differ from the literature-based matrix, suggesting that further analysis is required.


Slide #12: 
Executive Summary
3
Overview
04


Slide #13: 
04
Next Steps
Thank you for your collaboration thus far! We look forward to working with you in Spring 2021.
Matrix-based prediction framework
Apply this method to UNIDO’s most recent internal IRPF data.
COVID-19
Investigate the impact of COVID-19 on progress towards the SDGs via the IRPF.
Communicate results
Update reports and visualizations on the UNIDO Open Data Platform.",,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/F20_SIMON_Final.pptx,F20,,"Slide #0: 
Fintech Automation: Using and Validating Data More Effectively  
Semester Project Overview
HDAG Group 2020 ® All Rights Reserved.

 Image: a black and white photo of a sign on a white wall



Slide #1: 
Executive Summary
3
Overview
01


Slide #2: 
Automated PDF parsing 
5
Convert vendor specific pdf formats to excel in one click 
Automatically merge calendar offerings from different sources
Replace manual entry process to save time and avoid errors



 Image: a newspaper with a picture of a person sitting in a chair


 Image: a collage of photos showing different types of electronic devices



Slide #3: 
Quickly merge network calendars

 Image: a computer screen with a bunch of numbers on it


 Image: a collage of photos showing different types of equipment


 Image: a collage of photos showing different types of food



Slide #4: 
Executive Summary
3
Overview
02


Slide #5: 
Use an NLP model to extract data from SEC website
12

 Image: a sign that is on top of a wall


 Image: a sign that is on the side of a building


 Image: a collage of images showing different types of electronic devices



Slide #6: 
Run once. Get an up to date DB.
12

 Image: a black and white image of a clock on a wall


 Image: an advertisement for a book called ""the book of life""",,,,,,
/mnt/c/Users/joshd/Downloads/pastsourcing/backend/data/presentations/F20_NYCCompany_Final.pptx,F20,"Slide #0: 
Hospitality Company & HDAG Case Study: A Streamlined Process for Updating a Travel Dashboard
Summary Deck
November 2020
HDAG Group 2020 ® All Rights Reserved.

 Image: a black and white photo of a sign on a white wall


 Image: a collage of images showing a sign and a person



Slide #1: 
1: Purpose
Goals: 
Develop a slide deck illustrating information from various travel-related data sources, public and private
Create visualizations, and a design that is easy to interpret
Semi-automate the whole process, so the main buffer in pumping out a new presentation is the updating of new data

‹#›
Solution: 
A dashboard with production integrated with both python and tableau scripts. A dashboard template and design scheme is preset, and all data can be updated and processed through a few clicks within python and tableau scripts to create new visuals. Those can be converted to the dashboard, to easily make a monthly report. 

Summary
Visual 
+
Description of Data
Data Source


Slide #2: 
2: Data, Workflow
Data Sources:
Unique sources amassed from both online (government, other free agencies), and private sources were used. Those include MTA, BLS, TSA, and additional confidential sources. 

Data Handling:
Each source has a unique script assigned, either through Python or Tableau scripts, for which visuals can be easily updated. They’re made to take in the strict format of how the data is given from the vendor.

Final Product:
Multi-page dashboard to be updated monthly. Pages assigned per category of relation to travel (i.e., Hotel Performance, Consumer Sentiment, Employment). 
‹#›


Slide #3: 
3: Example Visualizations
‹#›

 Image: a series of photos showing a variety of colorful kites


 Image: a collage of a picture of a computer screen



Slide #4: 
Challenge:
Hospitality Company: Master Summary
Project Guidelines:
Synthesize various data sources regarding travel information for Company City into a presentable format to forward to clients and serve as an effective informational tool. 
Technical Scope: Data Handling, Web Scraping, Data Visualization, Tableau Programming
Final Deliverable: 
A semi-auto process for updating a dashboard that integrates usage of Python and Tableau to created updated visuals with newly published data each month, from each unique vendor. With our process, client can create and update monthly dashboards of the quality we’ve given, without any of our help.

‹#›","Slide #0: 
Hospitality Company & HDAG Case Study: A Streamlined Process for Updating a Travel Dashboard
Summary Deck
November 2020
HDAG Group 2020 ® All Rights Reserved.

 Image: a black and white photo of a sign on a white wall


 Image: a collage of images showing a sign and a person



Slide #1: 
1: Purpose
Goals: 
Develop a slide deck illustrating information from various travel-related data sources, public and private
Create visualizations, and a design that is easy to interpret
Semi-automate the whole process, so the main buffer in pumping out a new presentation is the updating of new data

‹#›
Solution: 
A dashboard with production integrated with both python and tableau scripts. A dashboard template and design scheme is preset, and all data can be updated and processed through a few clicks within python and tableau scripts to create new visuals. Those can be converted to the dashboard, to easily make a monthly report. 

Summary
Visual 
+
Description of Data
Data Source


Slide #2: 
2: Data, Workflow
Data Sources:
Unique sources amassed from both online (government, other free agencies), and private sources were used. Those include MTA, BLS, TSA, and additional confidential sources. 

Data Handling:
Each source has a unique script assigned, either through Python or Tableau scripts, for which visuals can be easily updated. They’re made to take in the strict format of how the data is given from the vendor.

Final Product:
Multi-page dashboard to be updated monthly. Pages assigned per category of relation to travel (i.e., Hotel Performance, Consumer Sentiment, Employment). 
‹#›


Slide #3: 
3: Example Visualizations
‹#›

 Image: a series of photos showing a variety of colorful kites


 Image: a collage of a picture of a computer screen



Slide #4: 
Challenge:
Hospitality Company: Master Summary
Project Guidelines:
Synthesize various data sources regarding travel information for Company City into a presentable format to forward to clients and serve as an effective informational tool. 
Technical Scope: Data Handling, Web Scraping, Data Visualization, Tableau Programming
Final Deliverable: 
A semi-auto process for updating a dashboard that integrates usage of Python and Tableau to created updated visuals with newly published data each month, from each unique vendor. With our process, client can create and update monthly dashboards of the quality we’ve given, without any of our help.

‹#›",,,,,,
